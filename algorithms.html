<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>第 4 章 参数估计 | Master Thesis Template</title>
  <meta name="description" content="Spatial generalized linear mixed models, Stationary Spatial Gaussian Process, Stan platform, Markov chain Monte Carlo.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="第 4 章 参数估计 | Master Thesis Template" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/logo.png" />
  <meta property="og:description" content="Spatial generalized linear mixed models, Stationary Spatial Gaussian Process, Stan platform, Markov chain Monte Carlo." />
  <meta name="github-repo" content="XiangyunHuang/Thesis-Template-Bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 4 章 参数估计 | Master Thesis Template" />
  
  <meta name="twitter:description" content="Spatial generalized linear mixed models, Stationary Spatial Gaussian Process, Stan platform, Markov chain Monte Carlo." />
  <meta name="twitter:image" content="images/logo.png" />

<meta name="author" content="Xiang-Yun Huang">


<meta name="date" content="2019-01-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
<link rel="prev" href="models.html">
<link rel="next" href="simulations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">空间广义线性混合效应模型及其应用</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 绪论</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#reviews"><i class="fa fa-check"></i><b>1.1</b> 文献综述</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#stracture"><i class="fa fa-check"></i><b>1.2</b> 论文结构</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="prepare.html"><a href="prepare.html"><i class="fa fa-check"></i><b>2</b> 基础知识</a><ul>
<li class="chapter" data-level="2.1" data-path="prepare.html"><a href="prepare.html#sec:exp"><i class="fa fa-check"></i><b>2.1</b> 指数族</a></li>
<li class="chapter" data-level="2.2" data-path="prepare.html"><a href="prepare.html#sec:lse"><i class="fa fa-check"></i><b>2.2</b> 最小二乘估计</a></li>
<li class="chapter" data-level="2.3" data-path="prepare.html"><a href="prepare.html#sec:def-mle"><i class="fa fa-check"></i><b>2.3</b> 极大似然估计</a></li>
<li class="chapter" data-level="2.4" data-path="prepare.html"><a href="prepare.html#sec:stationary-gaussian-process"><i class="fa fa-check"></i><b>2.4</b> 平稳高斯过程</a></li>
<li class="chapter" data-level="2.5" data-path="prepare.html"><a href="prepare.html#sec:bayes-prior"><i class="fa fa-check"></i><b>2.5</b> 先验和后验分布</a></li>
<li class="chapter" data-level="2.6" data-path="prepare.html"><a href="prepare.html#sec:bayes-estimates"><i class="fa fa-check"></i><b>2.6</b> 常用贝叶斯估计</a></li>
<li class="chapter" data-level="2.7" data-path="prepare.html"><a href="prepare.html#sec:foundations"><i class="fa fa-check"></i><b>2.7</b> 本章小结</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>3</b> 统计模型</a><ul>
<li class="chapter" data-level="3.1" data-path="models.html"><a href="models.html#sec:Linear-Models"><i class="fa fa-check"></i><b>3.1</b> 简单线性模型</a></li>
<li class="chapter" data-level="3.2" data-path="models.html"><a href="models.html#sec:Generalized-Linear-Models"><i class="fa fa-check"></i><b>3.2</b> 广义线性模型</a></li>
<li class="chapter" data-level="3.3" data-path="models.html"><a href="models.html#sec:Generalized-Linear-Mixed-Effects-Models"><i class="fa fa-check"></i><b>3.3</b> 广义线性混合效应模型</a></li>
<li class="chapter" data-level="3.4" data-path="models.html"><a href="models.html#sec:Spatial-Generalized-linear-mixed-effects-models"><i class="fa fa-check"></i><b>3.4</b> 空间广义线性混合效应模型</a><ul>
<li class="chapter" data-level="3.4.1" data-path="models.html"><a href="models.html#subsec:structure-sglmm"><i class="fa fa-check"></i><b>3.4.1</b> 模型结构</a></li>
<li class="chapter" data-level="3.4.2" data-path="models.html"><a href="models.html#subsec:covariance-function"><i class="fa fa-check"></i><b>3.4.2</b> 自协方差函数</a></li>
<li class="chapter" data-level="3.4.3" data-path="models.html"><a href="models.html#subsec:identify"><i class="fa fa-check"></i><b>3.4.3</b> 模型识别</a></li>
<li class="chapter" data-level="3.4.4" data-path="models.html"><a href="models.html#subsec:prior-sglmm"><i class="fa fa-check"></i><b>3.4.4</b> 先验分布</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="models.html"><a href="models.html#sec:models"><i class="fa fa-check"></i><b>3.5</b> 本章小结</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="algorithms.html"><a href="algorithms.html"><i class="fa fa-check"></i><b>4</b> 参数估计</a><ul>
<li class="chapter" data-level="4.1" data-path="algorithms.html"><a href="algorithms.html#sec:mle"><i class="fa fa-check"></i><b>4.1</b> 极大似然估计</a></li>
<li class="chapter" data-level="4.2" data-path="algorithms.html"><a href="algorithms.html#sec:profile-likelihood"><i class="fa fa-check"></i><b>4.2</b> 剖面似然估计</a></li>
<li class="chapter" data-level="4.3" data-path="algorithms.html"><a href="algorithms.html#sec:Laplace-approximation"><i class="fa fa-check"></i><b>4.3</b> 拉普拉斯近似</a></li>
<li class="chapter" data-level="4.4" data-path="algorithms.html"><a href="algorithms.html#sec:algrithms"><i class="fa fa-check"></i><b>4.4</b> 参数估计的算法</a><ul>
<li class="chapter" data-level="4.4.1" data-path="algorithms.html"><a href="algorithms.html#subsec:LA"><i class="fa fa-check"></i><b>4.4.1</b> 拉普拉斯近似极大似然算法</a></li>
<li class="chapter" data-level="4.4.2" data-path="algorithms.html"><a href="algorithms.html#subsec:MCML"><i class="fa fa-check"></i><b>4.4.2</b> 蒙特卡罗极大似然算法</a></li>
<li class="chapter" data-level="4.4.3" data-path="algorithms.html"><a href="algorithms.html#sec:MCMC"><i class="fa fa-check"></i><b>4.4.3</b> 贝叶斯 Langevin-Hastings 算法</a></li>
<li class="chapter" data-level="4.4.4" data-path="algorithms.html"><a href="algorithms.html#LowRank"><i class="fa fa-check"></i><b>4.4.4</b> 低秩近似算法</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="algorithms.html"><a href="algorithms.html#sec:stan-hmc"><i class="fa fa-check"></i><b>4.5</b> 贝叶斯 Stan-HMC 算法</a><ul>
<li class="chapter" data-level="4.5.1" data-path="algorithms.html"><a href="algorithms.html#subsec:Curse-of-Dimensionality"><i class="fa fa-check"></i><b>4.5.1</b> 蒙特卡罗积分</a></li>
<li class="chapter" data-level="4.5.2" data-path="algorithms.html"><a href="algorithms.html#subsec:motivations"><i class="fa fa-check"></i><b>4.5.2</b> 背景和意义</a></li>
<li class="chapter" data-level="4.5.3" data-path="algorithms.html"><a href="algorithms.html#subsec:stan-samplers"><i class="fa fa-check"></i><b>4.5.3</b> Stan 简介</a></li>
<li class="chapter" data-level="4.5.4" data-path="algorithms.html"><a href="algorithms.html#subsec:stan-hmc"><i class="fa fa-check"></i><b>4.5.4</b> 实现过程</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="algorithms.html"><a href="algorithms.html#subsec:sglmm-with-r"><i class="fa fa-check"></i><b>4.6</b> 实现参数估计的 R 包</a></li>
<li class="chapter" data-level="4.7" data-path="algorithms.html"><a href="algorithms.html#sec:estimations"><i class="fa fa-check"></i><b>4.7</b> 本章小结</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simulations.html"><a href="simulations.html"><i class="fa fa-check"></i><b>5</b> 数值模拟</a><ul>
<li class="chapter" data-level="5.1" data-path="simulations.html"><a href="simulations.html#spatial-gaussian-processes"><i class="fa fa-check"></i><b>5.1</b> 平稳空间高斯过程</a><ul>
<li class="chapter" data-level="5.1.1" data-path="simulations.html"><a href="simulations.html#sim-one-gp"><i class="fa fa-check"></i><b>5.1.1</b> 一维平稳空间高斯过程</a></li>
<li class="chapter" data-level="5.1.2" data-path="simulations.html"><a href="simulations.html#sim-two-gp"><i class="fa fa-check"></i><b>5.1.2</b> 二维平稳空间高斯过程</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="simulations.html"><a href="simulations.html#sim-sglmm"><i class="fa fa-check"></i><b>5.2</b> 空间广义线性混合效应模型</a><ul>
<li class="chapter" data-level="5.2.1" data-path="simulations.html"><a href="simulations.html#sim-binomal-sglmm"><i class="fa fa-check"></i><b>5.2.1</b> 响应变量服从二项分布</a></li>
<li class="chapter" data-level="5.2.2" data-path="simulations.html"><a href="simulations.html#possion-sglmm"><i class="fa fa-check"></i><b>5.2.2</b> 响应变量服从泊松分布</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="simulations.html"><a href="simulations.html#sec:simulations"><i class="fa fa-check"></i><b>5.3</b> 本章小结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>6</b> 数据分析</a><ul>
<li class="chapter" data-level="6.1" data-path="applications.html"><a href="applications.html#sec:spatial-random-effects"><i class="fa fa-check"></i><b>6.1</b> 小麦产量的空间分布</a></li>
<li class="chapter" data-level="6.2" data-path="applications.html"><a href="applications.html#case-rongelap"><i class="fa fa-check"></i><b>6.2</b> 核污染浓度的空间分布</a></li>
<li class="chapter" data-level="6.3" data-path="applications.html"><a href="applications.html#sec:cases"><i class="fa fa-check"></i><b>6.3</b> 本章小结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>7</b> 总结与展望</a></li>
<li class="chapter" data-level="" data-path="ack.html"><a href="ack.html"><i class="fa fa-check"></i>致谢</a></li>
<li class="chapter" data-level="" data-path="author.html"><a href="author.html"><i class="fa fa-check"></i>作者简介</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>附录 A</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#tables-simulations"><i class="fa fa-check"></i>表格</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#simulate-code"><i class="fa fa-check"></i>代码</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">本论文由 bookdown 强力驱动</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Master Thesis Template</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="algorithms" class="section level1">
<h1><span class="header-section-number">第 4 章</span> 参数估计</h1>
<p>模型的参数估计是建模分析的重要步骤，鉴于空间广义线性混合效应模型（简称 SGLMM）的复杂性，文献中的参数估计方法，如最小二乘估计（简称 LSE）和极大似然估计（简称 MLE） 都没有显式的表达式，因此必须发展有效的算法。</p>
<p>目前，文献中的出现的算法有拉普拉斯近似极大似然算法、蒙特卡罗极大似然算法、贝叶斯Langevin-Hastings 算法和低秩近似算法。第<a href="algorithms.html#sec:mle">4.1</a>节，第<a href="algorithms.html#sec:profile-likelihood">4.2</a>节和第<a href="algorithms.html#sec:Laplace-approximation">4.3</a>节分别介绍 SGLMM 模型的极大似然估计，空间线性混合模型下的剖面似然估计和用于近似高维积分的拉普拉斯近似方法。似然函数卷入空间随机效应带来的高维积分，为此，文献中出现了三类估计模型参数的算法，分别是第 <a href="algorithms.html#subsec:LA">4.4.1</a> 小节介绍的拉普拉斯近似极大似然算法（简称LAML）、第 <a href="algorithms.html#subsec:MCML">4.4.2</a> 小节介绍的蒙特卡罗极大似然算法（简称 MCML）， 第 <a href="algorithms.html#sec:MCMC">4.4.3</a> 小节介绍的贝叶斯框架下的Langevin-Hastings算法 （简称贝叶斯 LH），第 <a href="algorithms.html#LowRank">4.4.4</a> 小节介绍的低秩近似算法（简称 Low-Rank）。</p>
<p>第 <a href="algorithms.html#sec:stan-hmc">4.5</a> 节详细介绍在贝叶斯 Langevin-Hastings 算法的基础上提出基于 Stan 实现的汉密尔顿蒙特卡罗算法（简称 Stan-HMC 算法），并分四个小节进行，第<a href="algorithms.html#subsec:motivations">4.5.2</a>小节介绍算法提出的背景和意义，第 <a href="algorithms.html#subsec:stan-samplers">4.5.3</a> 小节从 Stan 的发展、内置算法设置以及与同类软件的比较等三方面介绍，然后以数据集 Eight Schools 为例子介绍 Stan 的使用，为空间广义线性混合效应模型的 Stan 实现作铺垫，第<a href="algorithms.html#subsec:stan-hmc">4.5.4</a>小节介绍 Stan-HMC 算法实现过程。</p>
<div id="sec:mle" class="section level2">
<h2><span class="header-section-number">4.1</span> 极大似然估计</h2>
<p>设研究区域 <span class="math inline">\(D \subseteq \mathbb{R}^2\)</span>， 对于第 <span class="math inline">\(i\)</span> 次观测， <span class="math inline">\(s_i\)</span> 表示区域 <span class="math inline">\(D\)</span> 内的位置，<span class="math inline">\(y(s_i)\)</span> 表示响应变量，<span class="math inline">\(\mathbf{x}(s_i), i = 1, \ldots, n\)</span> 是一个 <span class="math inline">\(p\)</span> 维的固定效应，定义如下的 SGLMM 模型：
<span class="math display">\[
\mathrm{E}[y(s_i)|u(s_i)] = g^{-1}[\mathbf{x}(s_i)^{\top}\boldsymbol{\beta} + \mathbf{u}(s_i)], \quad i = 1, \ldots, n
\]</span></p>
<p>其中， <span class="math inline">\(g(\cdot)\)</span> 是实值可微的逆联系函数， <span class="math inline">\(\boldsymbol{\beta}\)</span> 是 <span class="math inline">\(p\)</span> 维的回归参数向量，代表 SGLMM 模型的固定效应。随机过程 <span class="math inline">\(\{U(\mathbf{s}): \mathbf{s} \in D\}\)</span> 是平稳的空间高斯过程，其均值为 <span class="math inline">\(\mathbf{0}\)</span>， 自协方差函数 <span class="math inline">\(\mathsf{Cov}(U(\mathbf{s}),U(\mathbf{s}&#39;)) = C(\mathbf{s} - \mathbf{s}&#39;; \boldsymbol{\theta})\)</span>， <span class="math inline">\(\boldsymbol{\theta}\)</span> 表示其中的参数向量。 <span class="math inline">\(\mathbf{u} = (u(s_1),u(s_2),\ldots,u(s_n))^{\top}\)</span> 是平稳空间高斯过程 <span class="math inline">\(U(\cdot)\)</span> 的一个实例。给定 <span class="math inline">\(\mathbf{u}\)</span>的情况下，观察值 <span class="math inline">\(\mathbf{y} = (y(s_1),y(s_2),\ldots,y(s_n))^{\top}\)</span> 是相互独立的。</p>
<p>给定 <span class="math inline">\(u_i = u(s_i), i = 1, \ldots, n\)</span>的条件下， <span class="math inline">\(y_i = y(s_i)\)</span> 的条件概率密度函数是
<span class="math display">\[
f(y_i|u_i;\boldsymbol{\beta}) = \exp[a(\mu_i)y_i - b(\mu_i)]c(y_i)
\]</span></p>
<p>其中， <span class="math inline">\(\mu_i = \mathsf{E}(y_i|u_i)\)</span>， <span class="math inline">\(a(\cdot),b(\cdot)\)</span> 和 <span class="math inline">\(c(\cdot)\)</span> 是特定的函数，具体的情况视所服从的分布而定，第<a href="prepare.html#prepare">2</a>章第<a href="prepare.html#sec:exp">2.1</a>节就不同的分布给出了不同函数形式。 SGLMM 模型的边际似然函数</p>
<p><span class="math display" id="eq:likelihood-function-1">\[\begin{equation}
L(\boldsymbol{\psi};\mathbf{y}) = \int \prod_{i=1}^{n} f(y_i|u_i;\boldsymbol{\beta})\phi_{n}(\mathbf{u};0,\Sigma_{\boldsymbol{\theta}})\mathrm{d}\mathbf{u} \tag{4.1}
\end{equation}\]</span></p>
<p>记号 <span class="math inline">\(\boldsymbol{\psi} = (\boldsymbol{\beta},\boldsymbol{\theta})\)</span> 表示 SGLMM 模型的全部参数， <span class="math inline">\(\phi_{n}(\cdot;0,\Sigma_{\boldsymbol{\theta}})\)</span> 表示 <span class="math inline">\(n\)</span> 元正态密度函数，其均值为 <span class="math inline">\(\mathbf{0}\)</span>， 协方差矩阵为 <span class="math inline">\(\Sigma_{\boldsymbol{\theta}} = (c_{ij}) = (C(s_i - s_j; \boldsymbol{\theta})), i,j = 1, \ldots, n\)</span>。 边际似然函数 <a href="algorithms.html#eq:likelihood-function-1">(4.1)</a> 几乎总是卷入一个难以处理的积分，这是主要面临的问题，并且计算量随观测 <span class="math inline">\(y_i\)</span> 的数量增加，此积分的维数等于观测点的个数。</p>
<p>再从贝叶斯方法的角度来看 SGLMM 模型， 令 <span class="math inline">\(\mathbf{y} = (y(s_1),\ldots,y(s_n))^{\top}\)</span> 表示观测值， <span class="math inline">\(\pi(\boldsymbol{\psi})\)</span> 表示模型参数的联合先验密度，那么联合后验密度为</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\pi(\boldsymbol{\psi},\mathbf{u}|\mathbf{y}) &amp;= \frac{f(\mathbf{y|\mathbf{u}, \boldsymbol{\psi}})\phi_{n}(\mathbf{u};0,\Sigma_{\boldsymbol{\theta}})\pi(\boldsymbol{\psi})}{m(\mathbf{y})} \\
m(\mathbf{y}) &amp;= \int f(\mathbf{y|\mathbf{u}, \boldsymbol{\psi}})\phi_{n}(\mathbf{u};0,\Sigma_{\boldsymbol{\theta}})\pi(\boldsymbol{\psi})\mathrm{d} \mathbf{u} \mathrm{d} \boldsymbol{\psi}
\end{aligned}
\end{equation}\]</span></p>
<p>同样遭遇难以处理的高维积分问题，所以 <span class="math inline">\(m(\mathbf{y})\)</span> 亦不会有显式表达式。特别地，若取 <span class="math inline">\(\pi(\boldsymbol{\psi})\)</span> 为扁平先验 （flat priors） ，如 <span class="math inline">\(\pi(\boldsymbol{\psi}) \propto 1\)</span>， 后验分布将简化为似然函数 <a href="algorithms.html#eq:likelihood-function-1">(4.1)</a> 的常数倍。 如果导出的后验是合适的， MCMC 算法可以用来研究似然函数， 但是对很多 SGLMM 模型扁平先验会导出不合适的后验 （improper posteriors） <span class="citation">(Natarajan and McCulloch <a href="#ref-Natarajan1995">1995</a>)</span>， 所以选用模糊先验 （diffuse prior）来导出合适的后验 （proper posteriors），导出的后验能接近似然函数，并不要求后验模 （posterior mode） 完全是似然函数的极大似然估计 MLE <span class="citation">(Kass and Wasserman <a href="#ref-Robert1996JASA">1996</a>)</span>。</p>
</div>
<div id="sec:profile-likelihood" class="section level2">
<h2><span class="header-section-number">4.2</span> 剖面似然估计</h2>
<p>极大似然估计是一种被广泛接受的参数估计方法，因其优良的大样本性质，在宽松的正则条件下，极大似然估计服从渐近正态分布，满足无偏性，而且是有效的估计。为了叙述方便，似然函数能有显式表达式，考虑空间线性混合效应模型，即响应变量服从正态分布的情况，以此来介绍剖面似然估计 （profile likelihood estimate）<span class="citation">(Peter J. Diggle and Ribeiro Jr., <a href="#ref-Diggle2007">n.d.</a>)</span>。
<span class="math display" id="eq:gaussian-model">\[\begin{equation}
\mathbf{Y} \sim \mathcal{N}(D\boldsymbol{\beta},\sigma^2 \mathbf{R}(\phi) + \tau^2\mathbf{I})
\tag{4.2}
\end{equation}\]</span>
其中， <span class="math inline">\(D\)</span> 是 <span class="math inline">\(n \times p\)</span> 的观测数据矩阵，<span class="math inline">\(\boldsymbol{\beta}\)</span> 是<span class="math inline">\(p\times 1\)</span>维的回归参数向量，<span class="math inline">\(\mathbf{R}\)</span> 依赖于 <span class="math inline">\(\phi\)</span>，这里<span class="math inline">\(\phi\)</span> 可能含有多个参数。模型 <a href="algorithms.html#eq:gaussian-model">(4.2)</a> 的对数似然函数
<span class="math display" id="eq:gauss-log-lik">\[\begin{equation}
\begin{aligned}
L(\boldsymbol{\beta},\tau^2,\sigma^2,\phi) = {} 
 &amp; - 0.5\{ n\log(2\pi) + \log\{|(\sigma^2\mathbf{R}(\phi)+\tau^2\mathbf{I})|\} \\
 &amp; + (\mathbf{Y} - D\boldsymbol{\beta})^{\top}(\sigma^2\mathbf{R}(\phi)+\tau^2\mathbf{I})^{-1}(\mathbf{Y} - D\boldsymbol{\beta}) \}  
\end{aligned} \tag{4.3}
\end{equation}\]</span>
极大化 <a href="algorithms.html#eq:gauss-log-lik">(4.3)</a> 式就是求模型 <a href="algorithms.html#eq:gaussian-model">(4.2)</a> 参数的极大似然估计，极大化对数似然的过程分步如下：</p>
<ol style="list-style-type: decimal">
<li>重参数化 <span class="math inline">\(\nu^2 = \tau^2/\sigma^2\)</span>，令 <span class="math inline">\(V = \mathbf{R}(\phi) + \nu^2 \mathbf{I}\)</span>；</li>
<li>给定 <span class="math inline">\(V\)</span>，对数似然函数 <a href="algorithms.html#eq:gauss-log-lik">(4.3)</a> 在
<span class="math display" id="eq:beta-sigma">\[\begin{equation}
\begin{aligned}
   \hat{\boldsymbol{\beta}}(V) &amp; =  (D^{\top} V^{-1} D)^{-1} D^{\top} V^{-1}\mathbf{Y} \\
   \hat{\sigma}^2(V)           &amp; =  n^{-1} \{\mathbf{Y} - D\hat{\boldsymbol{\beta}}(V)\}^{\top} V^{-1} \{\mathbf{Y} - D\hat{\boldsymbol{\beta}}(V)\}
\end{aligned} \tag{4.4}
\end{equation}\]</span>
取得极大值；</li>
<li>将 <a href="algorithms.html#eq:beta-sigma">(4.4)</a> 式代入对数似然函数 <a href="algorithms.html#eq:gauss-log-lik">(4.3)</a> 式，可获得一个简化的对数似然
<span class="math display" id="eq:reduced-gauss-log-lik">\[\begin{equation}
   L_{0}(\nu^2,\phi) = - 0.5\{ n\log(2\pi) + n\log \hat{\sigma}^2(V) + \log |V| + n \} \tag{4.5}
\end{equation}\]</span></li>
<li>关于参数 <span class="math inline">\(\nu^2, \phi\)</span> 极大化 <a href="algorithms.html#eq:reduced-gauss-log-lik">(4.5)</a> 式，获得参数 <span class="math inline">\(\nu^2, \phi\)</span> 的估计值，再将其回代 <a href="algorithms.html#eq:beta-sigma">(4.4)</a> 式，获得估计值 <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> 和 <span class="math inline">\(\hat{\sigma}^2\)</span>。</li>
</ol>
<p>在空间线性混合效应模型的设置下，上述极大化似然函数的过程可能与自协方差函数的类型有关，如在使用 Matérn 型自协方差函数的时，平滑参数 <span class="math inline">\(\kappa\)</span> 也卷入到 <span class="math inline">\(\phi\)</span> 中，导致识别问题。因此，让 <span class="math inline">\(\kappa\)</span> 分别取 <span class="math inline">\(0.5,1.5,2.5\)</span>，使得平稳空间高斯过程 <span class="math inline">\(\mathcal{S}\)</span> 覆盖到不同程度的均方可微性 <span class="citation">(Warnes and Ripley <a href="#ref-Warnes1987">1987</a>)</span>。原则上，极大似然估计的变化情况可以通过观察对数似然函数的曲面来分析<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>，但是，似然曲面的维数往往不允许直接观察。在这种情形下，另一个基于似然的想法是剖面似然 （profile likelihood）。一般地，假定有一个模型含有参数 <span class="math inline">\((\alpha,\phi)\)</span>，其参数的似然函数表示为 <span class="math inline">\(L(\alpha,\phi)\)</span>。则关于 <span class="math inline">\(\alpha\)</span> 的剖面似然函数定义为
<span class="math display" id="eq:profile-log-lik">\[\begin{equation}
L_{p}(\alpha) = L(\alpha,\hat{\psi}(\alpha)) = \max_{\psi} (L(\alpha,\psi))
\tag{4.6}
\end{equation}\]</span>
即考虑似然函数随 <span class="math inline">\(\alpha\)</span> 的变化情况，对每一个 <span class="math inline">\(\alpha\)</span> （保持 <span class="math inline">\(\alpha\)</span> 不变），指定 <span class="math inline">\(\psi\)</span> 的值使得对数似然取得最大值。剖面似然就是让我们可以观察到关于 <span class="math inline">\(\alpha\)</span> 的似然曲面，显然，其维数比完全似然曲面要低，与只有一个参数的对数似然一样，它也可以用来计算单个参数的置信区间。现在，注意到简化的对数似然 <a href="algorithms.html#eq:reduced-gauss-log-lik">(4.5)</a> 其实可以看作模型 <a href="algorithms.html#eq:gaussian-model">(4.2)</a> 关于 <span class="math inline">\((\nu^2,\phi)\)</span> 的剖面对数似然 <span class="citation">(Peter J. Diggle and Ribeiro Jr., <a href="#ref-Diggle2007">n.d.</a>)</span>。</p>
</div>
<div id="sec:Laplace-approximation" class="section level2">
<h2><span class="header-section-number">4.3</span> 拉普拉斯近似</h2>
<p>先回顾一下基本的泰勒展开，将一个函数 <span class="math inline">\(f(x)\)</span> 在点 <span class="math inline">\(a\)</span> 处展开成和的形式，有时候是无穷多项，可以使用其中的有限项作为近似，通常会选用前三项，即到达函数 <span class="math inline">\(f(x)\)</span> 二阶导的位置。
<span class="math display">\[ f(x) = f(a) + \frac{f&#39;(a)}{1!}(x-a) + \frac{f&#39;&#39;(a)}{2!}(x-a)^2 + \frac{f&#39;&#39;&#39;(a)}{3!}(x-a)^3 + \ldots \]</span>
以基本的抛物线函数 <span class="math inline">\(f(x) = x^2\)</span> 为例，考虑将它在 <span class="math inline">\(a = 2\)</span> 处展开。首先计算 <span class="math inline">\(f(x)\)</span> 的各阶导数
<span class="math display">\[\begin{align*}
   &amp;{} f(x) = x^2, \quad f&#39;(x) = 2x, \\
   &amp;{} f&#39;&#39;(x) = 2 \quad f^{(n)}(x) = 0, \quad n = 3,4,\ldots
\end{align*}\]</span>
因此，<span class="math inline">\(f(x)\)</span> 可以展开成有限项的和的形式
<span class="math display">\[ f(x) = x^2 = 2^2 + 2(2)(x-2) + \frac{2}{2}(x-2)^2 \]</span>
拉普拉斯近似本质上是用正态分布来近似任意分布 <span class="math inline">\(g(x)\)</span>，用泰勒展开的前三项近似 <span class="math inline">\(\log g(x)\)</span>，展开的位置是密度函数 <span class="math inline">\(g(x)\)</span> 的极值点 <span class="math inline">\(\hat{x}\)</span>，则有
<span class="math display">\[ \log g(x) \approx \log g(\hat{x}) + \frac{\partial \log g(\hat{x})}{\partial x} (x - \hat{x}) + \frac{\partial^2 \log g(\hat{x})}{2\partial x^2} (x - \hat{x})^2 \]</span>
由于是在函数 <span class="math inline">\(g(x)\)</span> 的极值点 <span class="math inline">\(\hat{x}\)</span> 展开， 所以 <span class="math inline">\(x = \hat{x}\)</span> 一阶导是 0，用曲率去估计方差是 <span class="math inline">\(\hat{\sigma}^2 = -1/\frac{\partial^2 \log g(\hat{x})}{2\partial x^2}\)</span>，再重写上述近似
<span class="math display">\[ \log g(x) \approx \log g(\hat{x}) - \frac{1}{2\hat{\sigma}^2} (x - \hat{x})^2 \]</span>
现在，用这个结果做正态近似，将上式两端先取指数，再积分，移去常数项
<span class="math display">\[ \int g(x) \mathrm{d}x = \int \exp[\log g(x)] \mathrm{d}x \approx \mathrm{constant} \int \exp[- \frac{(x - \hat{x})^2}{2\hat{\sigma}^2}] \mathrm{d}x \]</span>
则拉普拉斯方法近似任意密度函数 <span class="math inline">\(g(x)\)</span> 得到的正态分布的均值为 <span class="math inline">\(\hat{x}\)</span>， <span class="math inline">\(\hat{x}\)</span> 可以通过求解方程 <span class="math inline">\(g&#39;(x) = 0\)</span> 获得，方差为 <span class="math inline">\(\hat{\sigma}^2 = -1/g&#39;&#39;(\hat{x})\)</span>。下面以卡方分布 <span class="math inline">\(\chi^2\)</span> 为例，由于
<span class="math display">\[\begin{align*}
    &amp;{} f(x; k)     = \frac{ x^{k/2-1} \mathrm{e}^{-x/2} }{ 2^{k/2}\Gamma(k/2) }, x \geq 0 \quad
        \log f(x)   = (k/2 - 1) \log x - x/2 \\
    &amp;{} \log f&#39;(x)  = (k/2-1)/x - 1/2 = 0 \\
    &amp;{} \log f&#39;&#39;(x) = -(k/2-1)/x^2
\end{align*}\]</span>
所以，卡方分布的拉普拉斯近似为
<span class="math display">\[ \chi_{k}^2 \overset{LA}{\sim}  \mathcal{N}(\hat{x} = k-2, \hat{\sigma}^2 = 2(k-2)) \]</span>
且自由度越大，近似效果越好，对于多元分布的情况不难推广，使用多元泰勒展开和黑塞矩阵即可表示<span class="citation">(Tierney and Kadane <a href="#ref-Tierney1986">1986</a>)</span>。</p>
</div>
<div id="sec:algrithms" class="section level2">
<h2><span class="header-section-number">4.4</span> 参数估计的算法</h2>
<div id="subsec:LA" class="section level3">
<h3><span class="header-section-number">4.4.1</span> 拉普拉斯近似极大似然算法</h3>
<p>为描述拉普拉斯近似极大似然算法，空间广义线性混合效应模型的结构重新表述如下：</p>
<p><span class="math display" id="eq:sglmm">\[\begin{equation}
\begin{aligned}
\mathbf{Y(x)} | S(\mathbf{x}) &amp; \sim  f(\cdot;\boldsymbol{\mu(x)},\psi) \\
g(\boldsymbol{\mu}(\mathbf{x})) &amp; =  D\boldsymbol{\beta} + S(\mathbf{x}) 
                         = D\boldsymbol{\beta} + \sigma R(\mathbf{x};\phi) + \tau z \\
S(\mathbf{x}) &amp; \sim  \mathcal{N}(\mathbf{0},\Sigma)
\end{aligned} \tag{4.7}
\end{equation}\]</span></p>
<p>SGLMM 模型假定在给定高斯空间过程 <span class="math inline">\(S(\mathbf{x})\)</span> 的条件下， <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> 是独立的，并且服从分布 <span class="math inline">\(f(\cdot;\boldsymbol{\mu}(\mathbf{x}),\psi)\)</span>。此分布的参数有两个来源，其一是与联系函数 <span class="math inline">\(g\)</span> 关联的线性预测 <span class="math inline">\(\boldsymbol{\mu}(\mathbf{x})\)</span>，其二是密度分布函数 <span class="math inline">\(f\)</span> 的发散参数 <span class="math inline">\(\psi\)</span>，可以看作是似然函数中的附加参数。空间过程 <span class="math inline">\(S(\mathbf{x})\)</span> 分解为空间相关 <span class="math inline">\(R(\mathbf{x};\phi)\)</span> 和独立过程 <span class="math inline">\(Z\)</span>，二者分别被参数 <span class="math inline">\(\sigma\)</span> 和 <span class="math inline">\(\tau\)</span> 归一化而具有单位方差。线性预测包含一组固定效应 <span class="math inline">\(D\boldsymbol{\beta}\)</span>，空间相关的随机效应 <span class="math inline">\(R(\mathbf{x};\phi)\)</span>，与空间不相关的随机效应 <span class="math inline">\(\tau z \sim \mathcal{N}(\mathbf{0},\tau^2\mathbf{I})\)</span>。<span class="math inline">\(D\)</span> 是根据协变量观测值得到的数据矩阵，<span class="math inline">\(\boldsymbol{\beta}\)</span> 是 <span class="math inline">\(p \times 1\)</span> 维的回归参数向量。</p>
<p><span class="math inline">\(R(\mathbf{x};\phi)\)</span> 是具有单位方差的平稳空间高斯过程，其自相关函数为 <span class="math inline">\(\rho(u,\phi)\)</span>，这里 <span class="math inline">\(u\)</span> 表示一对空间位置之间的距离，<span class="math inline">\(\phi\)</span> 是刻画空间相关性的参数。自相关函数 <span class="math inline">\(\rho(u,\phi) (\in \mathbb{R}^d)\)</span> 是 <span class="math inline">\(d\)</span> 维空间到一维空间的映射函数，特别地，假定空间过程 <span class="math inline">\(S(\mathbf{x})\)</span>的自相关函数仅仅依赖成对点之间的欧氏距离，即 <span class="math inline">\(u =\|x_i - x_j\|\)</span>。常见的自相关函数有指数型、梅隆型和球型。线性预测的随机效应部分协方差矩阵 <span class="math inline">\(\Sigma = \sigma^2 R(\mathbf{x};\phi) + \tau^2\mathbf{I}\)</span>。</p>
<p>估计 SGLMM 模型 <a href="algorithms.html#eq:sglmm">(4.7)</a> 的参数 <span class="math inline">\(\boldsymbol{\theta} = (\boldsymbol{\beta},\sigma^2,\tau^2,\phi,\psi)\)</span> 需要极大化边际似然函数</p>
<p><span class="math display" id="eq:marginal-likelihood">\[\begin{equation}
L(\boldsymbol{\theta};\mathbf{y}) = \int_{\mathbb{R}^n} [\mathbf{Y(x)}|S(\mathbf{x})][S(\mathbf{x})]\mathrm{d}S(\mathbf{x}) \tag{4.8}
\end{equation}\]</span></p>
<p>边际似然函数 <span class="math inline">\(L(\boldsymbol{\theta};\mathbf{y})\)</span> 包含两个分布的乘积和随机效应的积分，并且这个积分无法显式的表示，第一个分布是观测变量 <span class="math inline">\(\mathbf{Y}\)</span> 的抽样分布，第二个分布是多元高斯分布。一个特殊的情况是 <span class="math inline">\(\mathbf{Y}\)</span> 也假设服从多元高斯分布，这时积分有显式表达式。</p>
<p>边际似然函数 <a href="algorithms.html#eq:marginal-likelihood">(4.8)</a> 卷入的数值积分是充满挑战的，因为积分的维数 <span class="math inline">\(n\)</span> 是观测值的数目，所以像二次、高斯-埃尔米特或适应性高斯-埃尔米特数值积分方式都是不可用的，Tierney 和 Kadane （1986 年）提出拉普拉斯近似方法 <span class="citation">(Tierney and Kadane <a href="#ref-Tierney1986">1986</a>)</span>，它在纵向数据分析中被大量采用 <span class="citation">(Peter J. Diggle et al., <a href="#ref-Diggle2002Analysis">n.d.</a>)</span>。总之，对空间广义线性混合效应模型而言，拉普拉斯近似还可以继续采用，想法是近似边际似然函数中的高维 <span class="math inline">\((n &gt; 3)\)</span> 积分，获得一个易于处理的表达式，有了积分的显式表达式，就可以用数值的方法求边际似然函数的极大值。拉普拉斯方法即用如下方式近似 <a href="algorithms.html#eq:marginal-likelihood">(4.8)</a> 中的积分</p>
<p><span class="math display" id="eq:laplace-approximate">\[\begin{equation}
I   =  \int_{\mathbb{R}^n} \exp\{Q(\mathbf{s})\}\mathrm{d}\mathbf{s} 
  \approx  (2\pi)^{n/2} |-Q&#39;&#39;(\hat{\mathbf{s}})|^{-1/2}\exp\{Q(\hat{\mathbf{s}})\} \tag{4.9}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(Q(\mathbf{s})\)</span> 为已知的 <span class="math inline">\(n\)</span> 元函数，<span class="math inline">\(\hat{\mathbf{s}}\)</span> 是其极大值点，<span class="math inline">\(Q&#39;&#39;(\hat{\mathbf{s}})\)</span> 是黑塞矩阵。拉普拉斯近似的一维情形和主要近似的想法已在第<a href="prepare.html#prepare">2</a>章第<a href="algorithms.html#sec:Laplace-approximation">4.3</a>节详细阐述。</p>
<p>拉普拉斯近似方法也可以用于一般的广义线性混合效应模型的似然推断，特别地，对于空间广义线性混合效应模型，假定条件分布 <span class="math inline">\(f\)</span> 可以表示成如下指数族的形式</p>
<p><span class="math display" id="eq:exponential-family">\[\begin{equation}
f(\mathbf{y};\boldsymbol{\beta})  = \exp\{\mathbf{y}^{\top} (D\boldsymbol{\beta} + S(\mathbf{x})) - \mathbf{1}^{\top} b( D\boldsymbol{\beta} + S(\mathbf{x})) + \mathbf{1}^{\top} c(\mathbf{y}) \}  \tag{4.10}
\end{equation}\]</span></p>
<p>其中 <span class="math inline">\(b(\cdot)\)</span> 是特定的函数，常用的分布有泊松分布和二项分布等，详见第<a href="prepare.html#prepare">2</a>章第<a href="prepare.html#sec:exp">2.1</a>节。把 <a href="algorithms.html#eq:marginal-likelihood">(4.8)</a> 式中关于 <span class="math inline">\([S(\mathbf{x})]\)</span> 多元高斯密度函数表示为</p>
<p><span class="math display" id="eq:multi-gaussian-dist">\[\begin{align}
f(S(\mathbf{x});\Sigma) &amp; = (2\pi)^{-n/2}|\Sigma|^{-1/2} \exp\{ -\frac{1}{2}S(\mathbf{x})^{\top} \Sigma^{-1} S(\mathbf{x}) \} \\
                      &amp; = \exp\{ - \frac{n}{2}\log (2\pi) -\frac{1}{2}\log |\Sigma|  -\frac{1}{2}S(\mathbf{x})^{\top} \Sigma^{-1} S(\mathbf{x}) \} \tag{4.11}
\end{align}\]</span></p>
<p>现在将边际似然函数 <a href="algorithms.html#eq:marginal-likelihood">(4.8)</a> 写成适合使用拉普拉斯近似的格式</p>
<p><span class="math display">\[\begin{equation}
L(\boldsymbol{\theta};\mathbf{y}) = \int_{\mathbb{R}^n} \exp\{Q(S(\mathbf{x}))\} \mathrm{d}S(\mathbf{x}) 
\end{equation}\]</span></p>
<p>其中</p>
<p><span class="math display" id="eq:log-lik">\[\begin{equation}
\begin{aligned}
Q(S(\mathbf{x})) ={} &amp;  \mathbf{y}^{\top} (D \boldsymbol{\beta} + S(\mathbf{x})) - \mathbf{1}^{\top} b(D \boldsymbol{\beta} + S(\mathbf{x})) + \mathbf{1}^{\top}c(\mathbf{y}) \\
                   &amp; - \frac{n}{2}\log (2\pi) -\frac{1}{2}\log |\Sigma| -\frac{1}{2}S(\mathbf{x})^{\top} \Sigma^{-1} S(\mathbf{x})
\end{aligned} \tag{4.12}
\end{equation}\]</span></p>
<p>方程 <a href="algorithms.html#eq:log-lik">(4.12)</a> 凸显了采纳拉普拉斯近似方法拟合空间广义线性混合效应模型的方便性，可以把 <a href="algorithms.html#eq:log-lik">(4.12)</a> 当成两部分来看，前一部分是广义线性模型下样本的对数似然的和的形式，后一部分是多元高斯分布的对数似然。要使用 <a href="algorithms.html#eq:laplace-approximate">(4.9)</a> 式，需要函数 <span class="math inline">\(Q(S(\mathbf{x}))\)</span> 的极大值点 <span class="math inline">\(\hat{\mathbf{s}}\)</span>，这里采用牛顿-拉夫森算法 （Newton-Raphson，简称 NR） 寻找 <span class="math inline">\(n\)</span> 元函数的极大值点，NR 算法需要重复计算</p>
<p><span class="math display">\[\begin{equation}
\mathbf{s}_{i+1} = \mathbf{s}_{i} - Q&#39;&#39;(\mathbf{s}_{i})^{-1}Q&#39;(\mathbf{s}_{i}) 
\end{equation}\]</span></p>
<p>一直收敛到 <span class="math inline">\(\hat{\mathbf{s}}\)</span>。在这个内迭代的过程中，将参数 <span class="math inline">\(\boldsymbol{\theta}\)</span> 当作已知的。<span class="math inline">\(Q\)</span> 函数的一阶和二阶导数如下</p>
<p><span class="math display" id="eq:second-deriv" id="eq:first-deriv">\[\begin{align}
Q&#39;(\mathbf{s})&amp; =  \{\mathbf{y} - b&#39;(D\boldsymbol{\beta} + \mathbf{s}) \}^{\top} - \mathbf{s}^{\top}\Sigma^{-1} \tag{4.13} \\
Q&#39;&#39;(\mathbf{s})&amp; =  -\mathrm{diag} \{b&#39;&#39;(D\boldsymbol{\beta} + \mathbf{s}) \} - \Sigma^{-1} \tag{4.14}
\end{align}\]</span></p>
<p>用拉普拉斯方法近似的对数似然<span class="math inline">\(\ell(\boldsymbol{\theta};\mathbf{y})\)</span></p>
<p><span class="math display" id="eq:approx-log-lik">\[\begin{equation}
\begin{aligned}
\ell(\boldsymbol{\theta};\mathbf{y}) = {} &amp; \frac{n}{2}\log (2\pi) -\frac{1}{2}\log | -\mathrm{diag} \{b&#39;&#39;(D\boldsymbol{\beta} + \mathbf{s}) \} - \Sigma^{-1} |  \\
&amp; + \mathbf{y}^{\top} (D\boldsymbol{\beta} + \hat{\mathbf{s}}) - \mathbf{1}^{\top} b( D\boldsymbol{\beta} + \hat{\mathbf{s}}) + \mathbf{1}^{\top} c(\mathbf{y}) \\
&amp; - \frac{n}{2}\log (2\pi) -\frac{1}{2}\log |\Sigma| -\frac{1}{2}\hat{\mathbf{s}}^{\top} \Sigma^{-1} \hat{\mathbf{s}}
\end{aligned} \tag{4.15}
\end{equation}\]</span></p>
<p>现在极大化近似对数似然 <a href="algorithms.html#eq:approx-log-lik">(4.15)</a> 式，此时是求模型参数，可称之为外迭代过程，常用的算法是 Broyden-Fletcher-Goldfarb-Shanno (简称BFGS) 算法，它内置在 R 函数 <code>optim()</code> 中。方便起见，模型参数记为 <span class="math inline">\(\boldsymbol{\theta} = (\boldsymbol{\beta},\log(\sigma^2),\log(\tau^2),\log(\phi),\log(\psi))\)</span>，且 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> 表示 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的最大似然估计，根据第<a href="prepare.html#prepare">2</a>章第<a href="prepare.html#sec:def-mle">2.3</a>节定理<a href="prepare.html#thm:asymptotic-normality">2.3</a>，则 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> 的渐进分布为</p>
<p><span class="math display">\[ \hat{\boldsymbol{\theta}} \sim \mathcal{N}(\boldsymbol{\theta}, \mathbf{I}_{o}^{-1}(\hat{\boldsymbol{\theta}})) \]</span></p>
<p>其中 <span class="math inline">\(\mathbf{I}_{o}(\hat{\boldsymbol{\theta}})\)</span> 为观察到的样本信息阵，注意到在空间广义线性混合效应模型下，不能直接计算 Fisher 信息阵，因为对数似然函数没有显式表达式，只有数值迭代获得在 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> 处的观测信息矩阵。通常，这类渐进近似对协方差参数 <span class="math inline">\(\sigma^2, \tau^2, \phi\)</span> 的估计效果不好，在数据集不太大的情形下，可用第 <a href="algorithms.html#sec:profile-likelihood">4.2</a> 节介绍的剖面似然方法计算协方差参数及其置信区间。剖面似然估计的算法实现过程详见 Bolker 等 （2017年） 开发的 bbmle 包 <span class="citation">(Bolker and R Development Core Team <a href="#ref-R-bbmle">2017</a>)</span>，下面给出计算的细节步骤：</p>
<ol style="list-style-type: decimal">
<li>选择模型参数 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的初始值 <span class="math inline">\(\boldsymbol{\theta}_{i}\)</span>；</li>
<li>计算协方差矩阵 <span class="math inline">\(\Sigma\)</span> 及其逆 <span class="math inline">\(\Sigma^{-1}\)</span>；</li>
<li>通过如下步骤极大<span class="math inline">\(Q\)</span>函数，获得估计值 <span class="math inline">\(\hat{\mathbf{s}}\)</span>；
<ol style="list-style-type: lower-alpha">
<li>为 <span class="math inline">\(\mathbf{s}\)</span> 选择初始值；</li>
<li>按 <a href="algorithms.html#eq:first-deriv">(4.13)</a> 式计算 <span class="math inline">\(Q&#39;(\mathbf{s})\)</span>，按 <a href="algorithms.html#eq:second-deriv">(4.14)</a> 式计算 <span class="math inline">\(Q&#39;&#39;(\mathbf{s})\)</span>，其中导数计算的代码实现可参考黄湘云（2016年） <span class="citation">(黄湘云 <a href="#ref-Huang2016COS">2016</a>)</span>；</li>
<li>解线性方程组 <span class="math inline">\(Q&#39;&#39;(\mathbf{s})\mathbf{s}^{\star} = Q&#39;(\mathbf{s})\)</span>；</li>
<li>更新 <span class="math inline">\(\mathbf{s = s + s^{\star}}\)</span>；</li>
<li>迭代直到收敛以获得 <span class="math inline">\(\hat{\mathbf{s}}\)</span>。</li>
</ol></li>
<li>用 <span class="math inline">\(\hat{\mathbf{s}}\)</span> 替换 <span class="math inline">\(S(\mathbf{x})\)</span>，在 <a href="algorithms.html#eq:log-lik">(4.12)</a> 式中计算 <span class="math inline">\(Q(\hat{\mathbf{s}})\)</span>；</li>
<li>用 <a href="algorithms.html#eq:laplace-approximate">(4.9)</a> 式计算积分的近似值，以获得边际似然 <a href="algorithms.html#eq:approx-log-lik">(4.15)</a> 式的值；</li>
<li>用 BFGS 算法获得下一个值 <span class="math inline">\(\boldsymbol{\theta}_{i+1}\)</span>；</li>
<li>重复上述过程直到收敛，获得参数的估计值 <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>。</li>
</ol>
<p>NR算法收敛速度是很快的，但是必须提供一个很好的初值，好的初值对于快速收敛到似然函数 <span class="math inline">\(\ell(\boldsymbol{\theta};\mathbf{y})\)</span> 的极大值点很重要。指定外迭代中的初值 <span class="math inline">\(\boldsymbol{\theta}_{0}\)</span>的一个策略是首先拟合一个简单的广义线性模型，获得回归系数 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的初值，基于这些值计算线性预测值 <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span>；然后计算残差 <span class="math inline">\(\hat{\boldsymbol{r}} = (\hat{\boldsymbol{\mu}} - \mathbf{y})\)</span>， <span class="math inline">\(\hat{\boldsymbol{r}}\)</span> 的方差作为 <span class="math inline">\(\sigma^2\)</span> 的初值，如果 SGLMM 带有块金效应，就用 <span class="math inline">\(\sigma^2\)</span> 的初值的一定比例，如 10% 作为 <span class="math inline">\(\tau^2\)</span> 的初值；最后，<span class="math inline">\(\phi\)</span> 的初值选择两个距离最大的观测点之间的距离的 10%，比较保险的办法是选择不同的 <span class="math inline">\(\phi\)</span> 作为初值，这个过程需要不断的试错以期获得算法的收敛<span class="citation">(Bonat and Ribeiro Jr. <a href="#ref-Bonat2016Practical">2016</a>)</span>。</p>
</div>
<div id="subsec:MCML" class="section level3">
<h3><span class="header-section-number">4.4.2</span> 蒙特卡罗极大似然算法</h3>
<p>为描述蒙特卡罗极大似然算法，空间广义线性混合效应模型的结构表述如下</p>
<p><span class="math display" id="eq:again-SGLMM">\[\begin{equation}
g(\mu_i) = T_{i} = d(x_i)^{\top}\boldsymbol{\beta} + S(x_i) + Z_i \tag{4.16}
\end{equation}\]</span></p>
<p>其中，令 <span class="math inline">\(d_{i} = d(x_i)^{\top}\)</span>，用<span class="math inline">\(d(x_i)^{\top}\)</span> 表示 主要是强调协变量的空间内容， 这里表示采样点处观测数据向量， 即 <span class="math inline">\(p\)</span> 个协变量在第 <span class="math inline">\(i\)</span> 个位置 <span class="math inline">\(x_i\)</span> 的观察值。 <span class="math inline">\(\mathcal{S} = \{S(x): x \in \mathbb{R}^2\}\)</span> 是均值为 <span class="math inline">\(\mathbf{0}\)</span>，方差为 <span class="math inline">\(\sigma^2\)</span>，平稳且各向同性的空间高斯过程，自相关函数是 <span class="math inline">\(\rho(u;\phi)\)</span>，<span class="math inline">\(S(x_i)\)</span> 表示空间效应，<span class="math inline">\(Z_i \stackrel{i.i.d}{\sim} \mathcal{N}(0,\tau^2)\)</span> 的块金效应，<span class="math inline">\(g\)</span> 是联系函数，<span class="math inline">\(x_i \in \mathbb{R}^2\)</span>是采样点的位置。 综上，模型 <a href="algorithms.html#eq:again-SGLMM">(4.16)</a> 待估计的参数有 <span class="math inline">\(\boldsymbol{\beta}\)</span> 和 <span class="math inline">\(\boldsymbol{\theta}&#39; = (\sigma^2,\phi,\tau^2)\)</span>。 特别地，当响应变量分别服从二项分布和泊松分布时， 模型 <a href="algorithms.html#eq:again-SGLMM">(4.16)</a> 分别变为模型 <a href="algorithms.html#eq:BL-SGLMM">(4.17)</a> 和模型 <a href="algorithms.html#eq:Poss-SGLMM">(4.18)</a>。
<span class="math display" id="eq:Poss-SGLMM" id="eq:BL-SGLMM">\[\begin{align}
\log\{\frac{p_i}{1-p_i}\} &amp; = T_{i} = d(x_i)^{\top}\boldsymbol{\beta} + S(x_i) + Z_i  \tag{4.17}\\
\log(\lambda_i)           &amp; = T_{i} = d(x_i)^{\top}\boldsymbol{\beta} + S(x_i) + Z_i  \tag{4.18}
\end{align}\]</span>
模型 <a href="algorithms.html#eq:BL-SGLMM">(4.17)</a> 中，响应变量 <span class="math inline">\(Y_i\)</span> 服从二项分布 <span class="math inline">\(Y_i \sim \mathrm{Binomial}(m_i,p_i)\)</span>， 均值 <span class="math inline">\(\mathsf{E}(Y_i|S(x_i),Z_i)=m_{i}p_{i}\)</span>， <span class="math inline">\(m_i\)</span> 表示在 <span class="math inline">\(x_i\)</span> 的位置抽取的样本量，总的样本量就是 <span class="math inline">\(M = \sum_{i=1}^{N}m_i\)</span>，<span class="math inline">\(N\)</span> 表示采样点的个数。模型 <a href="algorithms.html#eq:Poss-SGLMM">(4.18)</a> 中， 响应变量 <span class="math inline">\(Y_i\)</span> 服从泊松分布 <span class="math inline">\(Y_i \sim \mathrm{Poisson}(\lambda_i)\)</span>。 在获取响应变量 <span class="math inline">\(Y\)</span> 的观测的过程中，与广义线性或广义线性混合效应模型 <a href="models.html#eq:GLM">(3.2)</a> 和 <a href="models.html#eq:GLMM">(3.3)</a> 不同的在于：在每个采样点 <span class="math inline">\(x_i\)</span> 处，<span class="math inline">\(Y_i\)</span> 都服从参数不同但同类的分布。</p>
<p>模型 <a href="algorithms.html#eq:BL-SGLMM">(4.17)</a> 中参数 <span class="math inline">\(\boldsymbol{\beta}\)</span> 和 <span class="math inline">\(\boldsymbol{\theta}^{\top} = (\sigma^2,\phi,\tau^2)\)</span> 的似然函数是通过对 <span class="math inline">\(T_i\)</span> 内的随机效应积分获得的。 用大写 <span class="math inline">\(D\)</span> 表示 <span class="math inline">\(n\times p\)</span> 的数据矩阵, <span class="math inline">\(y = (y_1, y_2,\cdots, y_n)\)</span> 表示各空间位置 <span class="math inline">\(x_i\)</span> 处响应变量的观测值，对应模型 <a href="algorithms.html#eq:BL-SGLMM">(4.17)</a> 中的 <span class="math inline">\(Y_i \sim \mathrm{Binomial}(m_i,p_i)\)</span>， <span class="math inline">\(\mathbf{T} = (T_1,T_2,\ldots,T_n)\)</span> 的边际分布是 <span class="math inline">\(\mathcal{N}(D\boldsymbol{\beta}, \Sigma(\boldsymbol{\theta}))\)</span>， 其中，协方差矩阵 <span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span> 的对角元是 <span class="math inline">\(\sigma^2+\tau^2\)</span>， 非对角元是 <span class="math inline">\(\sigma^2\rho(u_{ij})\)</span>， <span class="math inline">\(u_{ij}\)</span> 是位置 <span class="math inline">\(x_i\)</span> 与 <span class="math inline">\(x_j\)</span> 之间的距离。在给定 <span class="math inline">\(\mathbf{T} = t = (t_1,t_2,\cdots,t_n)\)</span> 下， <span class="math inline">\(\mathbf{Y} = y =(y_1,\cdots,y_n)\)</span> 的条件分布是独立二项概率分布函数的乘积 <span class="math inline">\(f(y|t)=\prod_{i=1}^{n}f(y_{i}|t_{i})\)</span>， 因此， <span class="math inline">\(\boldsymbol{\beta}\)</span> 和 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的似然函数可以写成</p>
<p><span class="math display" id="eq:likelihood">\[\begin{equation}
L(\boldsymbol{\beta},\boldsymbol{\theta}) = f(y;\boldsymbol{\beta},\boldsymbol{\theta}) = \int_{\mathbb{R}^{n}}\mathcal{N}(t;D\boldsymbol{\beta},\Sigma(\boldsymbol{\theta}))f(y|t)dt \tag{4.19}
\end{equation}\]</span></p>
<p>其中<span class="math inline">\(\mathcal{N}(\cdot;\mu,\Sigma)\)</span> 表示均值为 <span class="math inline">\(\mu\)</span>，协方差矩阵为 <span class="math inline">\(\Sigma\)</span> 的多元高斯分布的密度函数。Geyer (1994 年)<span class="citation">(Geyer <a href="#ref-Geyer1994On">1994</a>)</span> 在给定 <span class="math inline">\(\mathbf{Y}=y\)</span> 的情况下，使用 <span class="math inline">\(\mathbf{T}\)</span> 的条件分布 <span class="math inline">\(f(\mathbf{T}|\mathbf{Y}=y)\)</span> 模拟近似方程 <a href="algorithms.html#eq:likelihood">(4.19)</a> 中的高维积分，则似然函数 <span class="math inline">\(L(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> 可以重写为</p>
<p><span class="math display" id="eq:likelihood2">\[\begin{equation}
\begin{aligned}
L(\boldsymbol{\beta},\boldsymbol{\theta})
&amp; = \int_{\mathbb{R}^{n}} \frac{\mathcal{N}(t;D\boldsymbol{\beta},\Sigma(\boldsymbol{\theta}))f(y|t)}{\mathcal{N}(t;D\boldsymbol{\beta}_{0},\Sigma(\boldsymbol{\theta}_{0}))f(y|t)}f(y,t)dt \\
&amp; \varpropto \int_{\mathbb{R}^{n}} \frac{\mathcal{N}(t;D\boldsymbol{\beta}, \Sigma(\boldsymbol{\theta}))}{\mathcal{N}(t;D\boldsymbol{\beta}_{0}, \Sigma(\boldsymbol{\theta}_{0}))}f(t|y)dt \\
&amp;= E_{T|y}\left[\frac{\mathcal{N}(t; D\boldsymbol{\beta}, \Sigma(\boldsymbol{\theta}))}{\mathcal{N}(t; D\boldsymbol{\beta}_{0}, \Sigma(\boldsymbol{\theta}_{0}))}\right] 
\end{aligned} \tag{4.20}
\end{equation}\]</span></p>
<p>其中 <span class="math inline">\(\boldsymbol{\beta}_{0},\boldsymbol{\theta}_{0}\)</span> 作为迭代初始值预先给定，则 <span class="math inline">\(Y\)</span> 和 <span class="math inline">\(T\)</span> 的联合分布可以表示成 <span class="math inline">\(f(y,t) = \mathcal{N}(t; D\boldsymbol{\beta}_{0}, \Sigma(\boldsymbol{\theta}_{0})) f(y|t)\)</span>。通过蒙特卡罗方法，用求和代替积分近似期望， 做法是从条件分布 <span class="math inline">\(f(T|Y=y; \boldsymbol{\beta}_0, \boldsymbol{\theta}_0)\)</span> 抽取 <span class="math inline">\(m\)</span> 个样本 <span class="math inline">\(t_{(i)}\)</span>， 那么，可以用方程 <a href="algorithms.html#eq:likelihood-approx">(4.21)</a> 近似方程 <a href="algorithms.html#eq:likelihood2">(4.20)</a></p>
<p><span class="math display" id="eq:likelihood-approx">\[\begin{equation}
L_{m}(\boldsymbol{\beta},\boldsymbol{\theta})=\frac{1}{m}\sum_{i=1}^{n}\frac{\mathcal{N}(t_{i};D\boldsymbol{\beta},\Sigma(\boldsymbol{\theta}))}{\mathcal{N}(t_{i};D\boldsymbol{\beta}_{0},\Sigma(\boldsymbol{\theta}_{0}))} \tag{4.21}
\end{equation}\]</span></p>
<p>这样做的依据是不管样本序列 <span class="math inline">\(t_{(i)}\)</span> 是否相关， <span class="math inline">\(L_{m}(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> 都是 <span class="math inline">\(L_{m}(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> 的一致估计 （consistent estimator）<span class="citation">(Giorgi and Diggle <a href="#ref-PrevMap2017JSS">2017</a>)</span>。 最优的 <span class="math inline">\(\boldsymbol{\beta}_0,\boldsymbol{\theta}_0\)</span> 是 <span class="math inline">\(\boldsymbol{\beta},\boldsymbol{\theta}\)</span> 的极大似然估计，即</p>
<p><span class="math display">\[
\max_{\boldsymbol{\beta},\boldsymbol{\theta}}L_{m}(\boldsymbol{\beta},\boldsymbol{\theta}) \rightarrow 1, m \rightarrow\infty
\]</span></p>
<p>既然初始值 <span class="math inline">\(\boldsymbol{\beta}_{0},\boldsymbol{\theta}_{0}\)</span> 与真实的极大似然估计值不同，可以用第 <span class="math inline">\(m\)</span> 步迭代获得的似然函数值 <span class="math inline">\(L_{m}(\hat{\boldsymbol{\beta}}_{m}, \hat{\boldsymbol{\theta})}_{m}\)</span> 与 1 的距离来刻画蒙特卡罗近似的准确度。实际操作中，用 <span class="math inline">\(\hat{\boldsymbol{\beta}}_{m}\)</span> 和 <span class="math inline">\(\hat{\boldsymbol{\theta}}_{m}\)</span> 表示最大化 <span class="math inline">\(L_{m}(\boldsymbol{\beta}, \boldsymbol{\theta})\)</span> 获得的 MCML 估计，重复迭代 <span class="math inline">\(\boldsymbol{\beta}_{0} = \hat{\boldsymbol{\beta}}_{m}\)</span> 和 <span class="math inline">\(\boldsymbol{\theta}_{0} = \hat{\boldsymbol{\theta}}_{m}\)</span> 直到收敛。 求蒙特卡罗近似的对数似然 <span class="math inline">\(l_{m}(\boldsymbol{\beta}, \boldsymbol{\theta}) = \log L_{m}(\boldsymbol{\beta}, \boldsymbol{\theta})\)</span> 的极值，可以使用 PrevMap 包，迭代 <span class="math inline">\(L_{m}(\boldsymbol{\beta}, \boldsymbol{\theta})\)</span> 的过程中，可以选择 BFGS 算法。由于 <span class="math inline">\(\boldsymbol{\psi}\)</span> 的似然曲面是相当扁平的，为了更好的收敛，做一步重参数化，即令 <span class="math inline">\(\boldsymbol{\psi} = \log(\boldsymbol{\theta})\)</span>，<span class="math inline">\(L_{m}(\boldsymbol{\beta}, \boldsymbol{\psi})\)</span> 关于 <span class="math inline">\(\boldsymbol{\beta}\)</span> 和 <span class="math inline">\(\boldsymbol{\psi}\)</span> 的一阶、二阶导数传递给 maxLik 包的 <code>maxBFGS</code> 函数。蒙特卡罗极大似然估计 <span class="math inline">\(\boldsymbol{\theta}_{m}\)</span> 的标准误差 （standard errors） 取似然函数 <span class="math inline">\(l_{m}(\boldsymbol{\beta},\boldsymbol{\theta})\)</span> 的负黑塞矩阵的逆的对角线元素的平方根。迭代次数足够多时，即 <span class="math inline">\(m\)</span> 充分大时，一般取到 10000 及以上，此时蒙特卡罗误差可忽略，即用方程 <a href="algorithms.html#eq:likelihood-approx">(4.21)</a> 近似 <a href="algorithms.html#eq:likelihood2">(4.20)</a> 的误差可忽略。</p>
</div>
<div id="sec:MCMC" class="section level3">
<h3><span class="header-section-number">4.4.3</span> 贝叶斯 Langevin-Hastings 算法</h3>
<p>在贝叶斯框架里，<span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{\theta}\)</span> 的后验分布由贝叶斯定理和 <span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{\theta}\)</span> 的联合先验分布确定，见第<a href="prepare.html#prepare">2</a>章基础知识第<a href="prepare.html#sec:bayes-prior">2.5</a>节后验分布的推导。假定 <span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{\theta}\)</span> 的先验分布如下：
<span class="math display">\[ \boldsymbol{\theta} \sim  g(\cdot), \quad \boldsymbol{\beta} | \sigma^2 \sim  \mathcal{N}(\cdot; \xi, \sigma^2 \Omega) \]</span>
其中 <span class="math inline">\(g(\cdot)\)</span> 可以是 <span class="math inline">\(\boldsymbol{\theta}\)</span> 的任意分布，<span class="math inline">\(\xi\)</span> 和 <span class="math inline">\(\Omega\)</span> 分别是 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的高斯先验的均值向量和协方差矩阵。则<span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{\theta}\)</span> 和 <span class="math inline">\(\mathbf{T}\)</span> 的后验分布是</p>
<p><span class="math display" id="eq:posterior">\[\begin{equation}
\pi(\boldsymbol{\beta}, \boldsymbol{\theta}, t | y) \propto g(\boldsymbol{\theta})\mathcal{N}(\boldsymbol{\beta}; \xi, \sigma^2 \Omega)\mathcal{N}(t; D\boldsymbol{\beta}, \Sigma(\boldsymbol{\theta}))f(y|t) \tag{4.22}
\end{equation}\]</span></p>
<p>R 包 PrevMap 内的函数 <code>binomial.logistic.Bayes</code> 可以从上述后验分布中抽得样本，这个抽样的过程使用了 MCMC 算法， <span class="math inline">\(\boldsymbol{\theta}, \boldsymbol{\beta}\)</span> 和 <span class="math inline">\(\mathbf{T}\)</span> 轮流迭代的过程如下：</p>
<ol style="list-style-type: decimal">
<li>初始化<span class="math inline">\(\boldsymbol{\beta}, \boldsymbol{\theta}\)</span> 和<span class="math inline">\(\mathbf{T}\)</span>；</li>
<li>对协方差<span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span>中的参数做如下变换 <span class="citation">(Christensen, Roberts, and Sköld <a href="#ref-Christensen2006">2006</a>)</span> <span class="math display">\[(\tilde{\theta}_{1}, \tilde{\theta}_{2}, \tilde{\theta}_{3}) = (\log \sigma, \log (\sigma^2/\phi^{2\kappa}), \log \tau^2)\]</span>
使用随机游走 Metropolis-Hastings 算法轮流更新上述三个参数，在第 <span class="math inline">\(i\)</span> 次迭代时，候选高斯分布的标准差 <span class="math inline">\(h\)</span> 是 <span class="math inline">\(h_{i} = h_{i-1} + c_{1}i^{-c_{2}}(\alpha_{i}-0.45)\)</span>，其中，<span class="math inline">\(c_{1} &gt; 0\)</span> 和 <span class="math inline">\(c_{2} \in (0,1]\)</span> 是预先给定的常数，<span class="math inline">\(\alpha_i\)</span> 是第 <span class="math inline">\(i\)</span> 次迭代时的接受概率，其中 0.45 是一元高斯分布的最优接受概率；</li>
<li>使用Gibbs步骤更新 <span class="math inline">\(\boldsymbol{\beta}\)</span>， 所需条件分布 <span class="math inline">\(\boldsymbol{\beta}|\boldsymbol{\theta},\mathbf{T}\)</span> 是高斯分布，均值 <span class="math inline">\(\tilde{\xi}\)</span>，协方差矩阵 <span class="math inline">\(\sigma^2\tilde{\Omega}\)</span>，且与 <span class="math inline">\(y\)</span> 不相关，记<span class="math inline">\(\Sigma(\boldsymbol{\theta}) = \sigma^2 R(\boldsymbol{\theta})\)</span> <span class="math display">\[ \tilde{\xi}  =  \tilde{\Omega}(\Omega^{-1}\xi+D^{\top} R(\boldsymbol{\theta})^{-1} \mathbf{T}), \quad \sigma^2 \tilde{\Omega}  =  \sigma^2(\Omega^{-1} + D^{\top} R(\boldsymbol{\theta})^{-1} D)^{-1} \]</span></li>
<li>使用汉密尔顿蒙特卡罗算法更新条件分布 <span class="math inline">\(\mathbf{T}|\boldsymbol{\beta},\boldsymbol{\theta},y\)</span>，用 <span class="math inline">\(H(t,u)\)</span> 表示汉密尔顿函数 <span class="math display">\[H(t, u) = u^{\top} u/2 - \log f(t | y, \boldsymbol{\beta}, \boldsymbol{\theta})\]</span>
其中，<span class="math inline">\(u\in\mathbb{R}^2\)</span>， <span class="math inline">\(f(t | y, \boldsymbol{\beta}, \boldsymbol{\theta})\)</span> 表示给定 <span class="math inline">\(\boldsymbol{\beta}\)</span>， <span class="math inline">\(\boldsymbol{\theta}\)</span> 和 <span class="math inline">\(y\)</span>下，<span class="math inline">\(\mathbf{T}\)</span> 的条件分布。根据汉密尔顿方程，函数 <span class="math inline">\(H(u, t)\)</span> 的偏导决定 <span class="math inline">\(u,t\)</span> 随时间 <span class="math inline">\(v\)</span> 的变化过程，
<span class="math display">\[\begin{eqnarray*}
\frac{d t_{i}}{d v} &amp; = &amp; \frac{\partial H}{\partial u_{i}} \\
\frac{d u_{i}}{d v} &amp; = &amp; -\frac{\partial H}{\partial t_{i}}
\end{eqnarray*}\]</span>
其中，<span class="math inline">\(i = 1,\ldots, n\)</span>， 上述动态汉密尔顿微分方程根据 leapfrog 方法<span class="citation">(Brooks et al. <a href="#ref-Brooks2011">2011</a>)</span>离散， 然后求解离散后的方程组获得近似解。</li>
</ol>
</div>
<div id="LowRank" class="section level3">
<h3><span class="header-section-number">4.4.4</span> 低秩近似算法</h3>
<p>低秩近似算法分两部分来阐述，第一部分讲空间高斯过程的近似，第二部分将该近似方法应用于 SGLMM 模型。</p>
<p>空间高斯过程 <span class="math inline">\(\mathcal{S} = \{S(x),x\in\mathbb{R}^2\}\)</span> 对任意给定一组空间位置 <span class="math inline">\(x_1,x_2,\ldots,x_n, \forall x_{i} \in \mathbb{R}^2\)</span>，随机变量 <span class="math inline">\(S(x_i),i = 1,2,\ldots,n\)</span> 的联合分布 <span class="math inline">\(\mathcal{S}=\{S(x_1),S(x_2),\ldots,S(x_n)\}\)</span> 是多元高斯分布，其由均值 <span class="math inline">\(\mu(x) = \mathsf{E}[S(x)]\)</span> 和协方差 <span class="math inline">\(G_{ij} =\gamma(x_i,x_j)= \mathsf{Cov}\{S(x_i),S(x_j)\}\)</span> 完全确定，即 <span class="math inline">\(\mathcal{S} \sim \mathcal{N}(\mu_{S},G)\)</span>。</p>
<p>低秩近似算法使用奇异值分解协方差矩阵 <span class="math inline">\(G\)</span> <span class="citation">(Peter J. Diggle and Ribeiro Jr., <a href="#ref-Diggle2007">n.d.</a>)</span>， 将协方差矩阵 <span class="math inline">\(G\)</span> 分解，也意味着将空间高斯过程 <span class="math inline">\(\mathcal{S}\)</span> 分解，令 <span class="math display">\[\mathcal{S} = AZ\]</span>
其中，<span class="math inline">\(A = U\Lambda^{1/2}\)</span>，对角矩阵 <span class="math inline">\(\Lambda\)</span> 包含 <span class="math inline">\(G\)</span> 的所有特征值，<span class="math inline">\(U\)</span> 是特征值对应的特征向量。将特征值按从大到小的顺序排列，取 <span class="math inline">\(A\)</span> 的前 <span class="math inline">\(m(&lt;n)\)</span> 列，即可获得 <span class="math inline">\(\mathcal{S}\)</span> 的近似 <span class="math inline">\(\mathcal{S}^{\star}\)</span>，
<span class="math display" id="eq:svd-S2">\[\begin{equation}
\mathcal{S}^{\star} = A_{m}Z \tag{4.23}
\end{equation}\]</span>
现在，<span class="math inline">\(Z\)</span> 只包含 <span class="math inline">\(m\)</span> 个相互独立的标准正态随机变量。方程 <a href="algorithms.html#eq:svd-S2">(4.23)</a> 可以表示成
<span class="math display" id="eq:svd-S3">\[\begin{equation}
\mathcal{S}^{\star} = \sum_{j=1}^{m}Z_{j}f_{j}(x_{i}), i = 1,2,\ldots,n \tag{4.24}
\end{equation}\]</span>
不难看出，方程<a href="algorithms.html#eq:svd-S3">(4.24)</a>不仅是 <span class="math inline">\(\mathcal{S}\)</span> 的低秩近似，还可用作为空间高斯过程 <span class="math inline">\(\mathcal{S}\)</span> 的定义。 更一般地，空间连续的随机过程 <span class="math inline">\(S(x)\)</span> 都可以表示成函数 <span class="math inline">\(f_{j}(x)\)</span> 和随机系数 <span class="math inline">\(A_{j}\)</span> 的线性组合。
<span class="math display" id="eq:svd-S4">\[\begin{equation}
S(x) = \sum_{j=1}^{m}A_{j}f_{j}(x), \forall x \in \mathbb{R}^2 \tag{4.25}
\end{equation}\]</span>
若 <span class="math inline">\(A_j\)</span> 服从零均值，协方差为 <span class="math inline">\(\mathsf{Cov}(A_{j},A_{k})=\gamma_{jk}\)</span> 的多元高斯分布，则 <span class="math inline">\(\mathcal{S}\)</span> 均值为0，协方差为
<span class="math display" id="eq:svd-S5">\[\begin{equation}
\mathsf{Cov}(S(x),S(x&#39;)) =  \sum_{j=1}^{m}\sum_{k=1}^{m}\gamma_{jk}f_{j}(x)f_{k}(x&#39;) \tag{4.26}
\end{equation}\]</span>
一般情况下，协方差结构 <a href="algorithms.html#eq:svd-S5">(4.26)</a> 不是平稳的，其中，<span class="math inline">\(f_{k}(\cdot)\)</span> 来自一组正交基
<span class="math display">\[\begin{equation*}
\int f_{j}(x)f_{k}(x)dx = 
\begin{cases}
1, &amp; i \neq j \\
0, &amp; i = j
\end{cases}
\end{equation*}\]</span>
随机系数 <span class="math inline">\(A_{j}\)</span> 满足相互独立。</p>
<p>为方便叙述起见，低秩近似算法以模型 <a href="algorithms.html#eq:lowrank-SGLMM">(4.27)</a> 为描述对象，它是模型 <a href="algorithms.html#eq:again-SGLMM">(4.16)</a> 的特殊形式，主要区别是模型 <a href="algorithms.html#eq:lowrank-SGLMM">(4.27)</a> 中，联系函数 <span class="math inline">\(g(\mu) = \log\big(\frac{\mu}{1-\mu}\big)\)</span></p>
<p><span class="math display" id="eq:lowrank-SGLMM">\[\begin{equation}
\log\{\frac{p_i}{1-p_i}\}  = T_{i} = d(x_i)^{\top}\boldsymbol{\beta} + S(x_i) + Z_{i} \tag{4.27}
\end{equation}\]</span></p>
<p>模型 <a href="algorithms.html#eq:lowrank-SGLMM">(4.27)</a> 中的高斯过程 <span class="math inline">\(\mathcal{S} = S(x)\)</span> 可以表示成高斯噪声的卷积形式</p>
<p><span class="math display" id="eq:convolution">\[\begin{equation}
S(x) = \int_{\mathbb{R}^2} K(\|x-t\|; \phi, \kappa) \: d B(t) \tag{4.28}
\end{equation}\]</span></p>
<p>其中，<span class="math inline">\(B\)</span> 表示布朗运动，<span class="math inline">\(\|\cdot\|\)</span> 表示欧氏距离，<span class="math inline">\(K(\cdot)\)</span> 表示自相关函数，其形如</p>
<p><span class="math display" id="eq:matern-kernel">\[\begin{equation}
K(u; \phi, \kappa) = \frac{\Gamma(\kappa + 1)^{1/2}\kappa^{(\kappa+1)/4}u^{(\kappa-1)/2}}{\pi^{1/2}\Gamma((\kappa+1)/2)\Gamma(\kappa)^{1/2}(2\kappa^{1/2}\phi)^{(\kappa+1)/2}}\mathcal{K}_{\kappa}(u/\phi), u &gt; 0 \tag{4.29}
\end{equation}\]</span></p>
<p>将方程 <a href="algorithms.html#eq:convolution">(4.28)</a>离散化，且让 <span class="math inline">\(r\)</span> 充分大，以获得低秩近似 <span class="citation">(Giorgi and Diggle <a href="#ref-PrevMap2017JSS">2017</a>)</span></p>
<p><span class="math display" id="eq:lr-approx">\[\begin{equation}
S(x) \approx \sum_{i = 1}^r K(\|x-\tilde{x}_{i}\|; \phi, \kappa) U_{i} \tag{4.30}
\end{equation}\]</span></p>
<p>式<a href="algorithms.html#eq:lr-approx">(4.30)</a>中， <span class="math inline">\((\tilde{x}_{1},\ldots,\tilde{x}_{r})\)</span> 表示空间网格的格点，<span class="math inline">\(U_{i}\)</span> 是独立同分布的高斯变量，均值为<span class="math inline">\(0\)</span>， 方差为<span class="math inline">\(\sigma^2\)</span>。 特别地， 尺度参数<span class="math inline">\(\phi\)</span>越大时，空间曲面越平缓，如图 <a href="models.html#fig:matern-2d">3.3</a>所示，在格点数 <span class="math inline">\(r\)</span> 比较少时也能得到不错的近似效果。 此外， 空间格点数 <span class="math inline">\(r\)</span> 与样本量 <span class="math inline">\(n\)</span> 是独立的， 因此， 低秩近似算法在样本量比较大时， 计算效率还比较高。</p>
<p>注意到平稳空间高斯过程 <span class="math inline">\(S(x)\)</span> 经过方程 <a href="algorithms.html#eq:lr-approx">(4.30)</a> 的近似已不再平稳。通过乘以 <span class="math display">\[\frac{1}{n}\sum_{i = 1}^n \sum_{j = 1}^m K(\|\tilde{x}_{j}-\tilde{x}_{i}\|; \phi, \kappa)^2\]</span> 来调整 <span class="math inline">\(\sigma^2\)</span>。 事实上，调整后的 <span class="math inline">\(\sigma^2\)</span> 会更接近于高斯过程 <span class="math inline">\(S(x)\)</span> 的实际方差。</p>
<p>低秩近似的关键是对高斯过程 <span class="math inline">\(\mathcal{S}\)</span> 的协方差矩阵 <span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span> 做降维分解， 这对 <span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span> 的逆和行列式运算是非常重要的，在计算之前，将 <span class="math inline">\(K(\boldsymbol{\theta})\)</span> 表示为 <span class="math inline">\(n\times r\)</span> 的核矩阵，它是由自协方差函数决定的空间距离矩阵，协方差矩阵 <span class="math inline">\(\Sigma(\boldsymbol{\theta}) = \sigma^2K(\boldsymbol{\theta})K(\boldsymbol{\theta})^{\top}+\tau^2 I_{n}\)</span>，<span class="math inline">\(I_{n}\)</span> 是 <span class="math inline">\(n\times n\)</span> 的单位矩阵。根据 Woodbury 公式可得 <span class="math display">\[\Sigma(\boldsymbol{\theta})^{-1} = \sigma^2\nu^{-2}(I_{n}-\nu^{-2}K(\boldsymbol{\theta})(\nu^{-2}K(\boldsymbol{\theta})^{\top} K(\boldsymbol{\theta})+I_{r})^{-1}K(\boldsymbol{\theta})^{\top})\]</span> 其中， <span class="math inline">\(\nu^2 = \tau^2/\sigma^2\)</span>，求 <span class="math inline">\(n\)</span> 阶方阵 <span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span> 的逆变成求 <span class="math inline">\(r\)</span> 阶方阵的逆， 从而达到了降维的目的。 下面再根据 Sylvester 行列式定理计算 <span class="math inline">\(\Sigma(\boldsymbol{\theta})\)</span> 的行列式 <span class="math inline">\(|\Sigma(\boldsymbol{\theta})|\)</span>
<span class="math display">\[\begin{eqnarray*}
|\Sigma(\boldsymbol{\theta})| &amp; = &amp; |\sigma^2K(\boldsymbol{\theta})K(\boldsymbol{\theta})^{\top}+\tau^2 I_{n}| \\ 
                 &amp; = &amp; \tau^{2n}|\nu^{-2}K(\boldsymbol{\theta})^{\top} K(\boldsymbol{\theta})+I_{r}|
\end{eqnarray*}\]</span>
类似地，行列式运算的维数从 <span class="math inline">\(n\times n\)</span> 降到了 <span class="math inline">\(r\times r\)</span> <span class="citation">(Peter J. Diggle and Ribeiro Jr., <a href="#ref-Diggle2007">n.d.</a>)</span>。</p>
</div>
</div>
<div id="sec:stan-hmc" class="section level2">
<h2><span class="header-section-number">4.5</span> 贝叶斯 Stan-HMC 算法</h2>
<div id="subsec:Curse-of-Dimensionality" class="section level3">
<h3><span class="header-section-number">4.5.1</span> 蒙特卡罗积分</h3>
<p>一般地，空间广义线性混合效应模型的统计推断总是不可避免的要面对高维积分，处理高维积分的方法一个是寻找近似方法避免求积分，一个是寻找有效的随机模拟方法直接求积分。这里，介绍蒙特卡罗方法求积分，以计算 <span class="math inline">\(N\)</span> 维超立方体的内切球的体积为例说明。</p>
<p>假设我们有一个 <span class="math inline">\(N\)</span> 维超立方体，其中心在坐标 <span class="math inline">\(\mathbf{0} = (0,\ldots,0)\)</span>。超立方体在点 <span class="math inline">\((\pm 1/2,\ldots,\pm 1/2)\)</span>，有 <span class="math inline">\(2^{N}\)</span> 个角落，超立方体边长是1，<span class="math inline">\(1^{N}=1\)</span>，所以它的体积是1。如果 <span class="math inline">\(N=1\)</span>，超立方体是一条从 <span class="math inline">\(-\frac{1}{2}\)</span> 到 <span class="math inline">\(\frac{1}{2}\)</span> 的单位长度的线，如果 <span class="math inline">\(N=2\)</span>，超立方体是一个单位正方形，对角是 <span class="math inline">\(\left( -\frac{1}{2}, -\frac{1}{2} \right)\)</span> 和 <span class="math inline">\(\left( \frac{1}{2}, \frac{1}{2} \right)\)</span>，如果 <span class="math inline">\(N=3\)</span>，超立方体就是单位体积的立方体，对角是 <span class="math inline">\(\left( -\frac{1}{2}, -\frac{1}{2}, -\frac{1}{2} \right)\)</span> 和 <span class="math inline">\(\left( \frac{1}{2}, \frac{1}{2}, \frac{1}{2} \right)\)</span>，依此类推，<span class="math inline">\(N\)</span> 维超立方体体积是1，对角是 <span class="math inline">\(\left( -\frac{1}{2}, \ldots, -\frac{1}{2} \right)\)</span> 和 <span class="math inline">\(\left( \frac{1}{2}, \ldots, \frac{1}{2} \right)\)</span>。</p>
<p>现在，考虑 <span class="math inline">\(N\)</span> 维超立方体的内切球，我们把它称为 <span class="math inline">\(N\)</span> 维超球，它的中心在原点，半径是 <span class="math inline">\(\frac{1}{2}\)</span>。我们说点 <span class="math inline">\(y\)</span> 在超球内，意味着它到原点的距离小于半径，即 <span class="math inline">\(\| y \| &lt; \frac{1}{2}\)</span>。一维情形下，超球是从的线，包含了整个超立方体。二维情形下，超球是中心在原点，半径为 <span class="math inline">\(\frac{1}{2}\)</span> 的圆。三维情形下，超球是立方体的内切球。已知单位超立方体的体积是1，但是其内的内切球的体积是多少呢？我们已经学过如何去定义一个积分计算半径为 <span class="math inline">\(r\)</span> 的二维球（即圆）的体积（即面积）是 <span class="math inline">\(\pi r^2\)</span>，三维情形下，内切球是 <span class="math inline">\(\frac{4}{3}\pi r^3\)</span>。但是更高维的欧式空间里，内切球的体积是多少呢？</p>
<p>在这种简单的体积积分设置下，当然可以去计算越来越复杂的多重积分，但是这里介绍采样的方法去计算积分，即所谓的蒙特卡罗方法，由梅特罗波利斯，冯<span class="math inline">\(\cdot\)</span>诺依曼和乌拉姆等在美国核武器研究实验室创立，当时正值二战期间，为了研制原子弹，出于保密的需要，与随机模拟相关的技术就代号蒙特卡罗。现在，蒙特卡罗方法占据现代统计计算的核心地位，特别是与贝叶斯相关的领域。</p>
<p>用蒙特卡罗方法去计算单位超立方体内的超球，首先需要在单位超立方体内产生随机点，然后计算落在超球内的点的比例，即超球的体积。随着点的数目增加，估计的体积会收敛到真实的体积。因为这些点都独立同均匀分布，根据中心极限定理，误差下降的比率是 <span class="math inline">\(\mathcal{O}\left( 1 / \sqrt{n} \right)\)</span>，这也意味着每增加一个小数点的准确度，样本量要增加 100 倍。</p>
<table>
<caption><span id="tab:calculate-volume-of-hyperball">表 4.1: </span> 前 10 维单位超立方体内切球的体积，超立方体内随机模拟的点的个数是 100000（已经四舍五入保留小数点后三位）</caption>
<thead>
<tr class="header">
<th align="left">维数</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">体积</td>
<td align="center">1.000</td>
<td align="center">0.784</td>
<td align="center">0.525</td>
<td align="center">0.307</td>
<td align="center">0.166</td>
<td align="center">0.081</td>
<td align="center">0.037</td>
<td align="center">0.016</td>
<td align="center">0.006</td>
<td align="center">0.0027</td>
</tr>
</tbody>
</table>
<p>表 <a href="algorithms.html#tab:calculate-volume-of-hyperball">4.1</a> 列出了前 10 维超球的体积，从上述计算过程中，我们发现随着维数增加，超球的体积迅速变小。这里有一个反直观的现象，内切球的体积竟然随着维数的增加变小，并且在 10 维的情形下，内切球的体积已不到超立方体的 0.3%，可以预见如果这个积分是 100 维甚至更多，那么内切球相比于正方体仅仅是一个极小的角落，随机点会越来越难以落在内切球内。甚至会因为所需要的随机数太多或者计算机资源的限制，而不可计算，开发更加高效的随机模拟算法也就势在必行。</p>
</div>
<div id="subsec:motivations" class="section level3">
<h3><span class="header-section-number">4.5.2</span> 背景和意义</h3>
<p>贝叶斯 MCMC 算法是一个计算密集型的算法，高效的实现对理论和应用都有非常重要的意义。因此，早在 1989 年剑桥大学研究人员开发出了 Windows 上的应用程序 WinBUGS，并被广泛使用。随着个人电脑的普及、Linux 和 MacOS 系统的蓬勃发展， 只能运行于 Windows 系统上的 WinBUGS 逐渐落后于时代，并在 2008 年宣布停止开发。 随后， OpenBUGS 以开源的开发方式重现了 WinBUGS 的功能，还可跨平台运行，弥补了 WinBUGS 的一些不足，而后又出现了同类的开源软件 JAGS。 无论是 OpenBUGS 还是 JAGS 都无法适应当代计算机硬件的迅猛发展， 它们的设计由于历史局限性， 已经无法满足在兼容性、 扩展性和高效性方面的要求。 所以， 哥伦比亚大学的统计系以开源的方式开发了新一代贝叶斯推断子程序库 Stan， 它与前辈们最明显的最直观的不同在于，它不是一个像 WinBUGS/OpenBUGS/JAGS 那样的软件有菜单窗口或软件内的命令行环境， <a href="http://mc-stan.org/">Stan</a> 是一种概率编程语言 <span class="citation">(Carpenter et al. <a href="#ref-Stan2017JSS">2017</a>)</span>， 可以替代 BUGS （Bayesian inference Using Gibbs Sampling） <span class="citation">(Lunn et al. <a href="#ref-BUGS2009">2009</a>)</span> 作为 MCMC 算法的高效实现。相比较于同类软件，Stan 的优势有：底层完全基于 C++ 实现；拥有活跃和快速发展的社区；支持在CPU/GPU上大规模并行计算；独立于系统和硬件平台；提供多种编程语言的接口，如 PyStan、 RStan 等等。 在大数据的背景下， 拥有数千台服务器的企业越来越多， 计算机资源达到前所未有的规模， 这为 Stan 的广泛应用打下了基础。</p>
</div>
<div id="subsec:stan-samplers" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Stan 简介</h3>
<p>在上世纪 40~50 年代，由梅特罗波利斯，冯<span class="math inline">\(\cdot\)</span>诺依曼和乌拉姆 （Stanislaw Ulam） 创立蒙特卡罗方法，为了纪念乌拉姆，Stan 就以他的名字命名。Stan 是一门基于 C++ 的概率编程语言，主要用于贝叶斯推断，它的代码完全<a href="http://mc-stan.org/">开源</a>的，托管在 <a href="https://github.com/stan-dev/stan">Github</a> 上，自 2012 年 8 月 30 日发布第一个 1.0 版本以来，截至写作时间已发布 33 个版本，目前最新版本是 2.18.0。使用 Stan，用户需提供数据、Stan 代码写的脚本模型，编译 Stan 写的程序，然后与数据一起运行，模型参数的后验模拟过程是自动实现的。除了可以在命令行环境下编译运行 Stan 脚本中写模型外，Stan 还提供其他编程语言的接口，如 R、Python、Matlab、Mathematica、Julia 等等，这使得熟悉其他编程语言的用户可以方便地调用和分析数据。但是，与 Python、R等 这类解释型编程语言不同， Stan 代码需要先翻译成 C++ 代码，然后使用系统编译器 （如 GCC） 编译，若使用 R 语言接口，编译后的动态链接库可以载入 R 内存中，再被其他 R 函数调用执行。</p>
<p>随机模拟的前提是有能产生高质量高效的伪随机数发生器，只有周期长，生成速度快，能通过一系列统计检验的伪随机数才能用作统计模拟。Stan 内置了 Mersenne Twister 发生器，它的周期长达 <span class="math inline">\(2^{19937}-1\)</span>，通过了一系列严格的检验，被广泛采用到现代软件中，如 Octave 和 Matlab 等 <span class="citation">(黄湘云 <a href="#ref-Huang2017COS">2017</a>)</span>。除了 Mersenne Twister 随机数发生器，Stan 还使用了 <a href="https://www.boost.org/">Boost C++</a> 和 <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen C++</a> 等模版库用于线性代数计算，这样的底层设计路线使得 Stan 的运算效率很高。</p>
<p>Stan 内置的采样器 No-U-Turn （简称 NUTS） 源于汉密尔顿蒙特卡罗算法 （Hamiltonian Monte Carlo，简称 HMC），最早由 Hoffman 和 Gelman （2014年） <span class="citation">(Hoffman and Gelman <a href="#ref-hoffman2014">2014</a>)</span> 提出。与 Stan 有相似功能的软件 BUGS 和 JAGS 主要采用的是 Gibbs 采样器，前者基于 Pascal 语言开发于 1989 年至 2004 年，后者基于 C++ 活跃开发于 2007 年至 2013 年。在时间上， Stan 具有后发优势，特别在灵活性和扩展性方面，它支持任意的目标函数，模型语言也更加简单易于推广学习，其每一行都是命令式的语句，而 BUGS 和 JAGS 采用声明式；在大量数据的建模分析中， Stan 可以更快地处理复杂模型，这一部分归功于它高效的算法实现和内存管理，另一部分在于高级的 MCMC 算法 — 带 NUTS 采样器的 HMC 算法。</p>
<p>Rubin （1981年） <span class="citation">(Rubin <a href="#ref-Rubin1981">1981</a>)</span> 分析了 Alderman 和 Powers <span class="citation">(Alderman and Powers <a href="#ref-Alderman1980">1980</a>)</span> 收集的原始数据，得出表 <a href="algorithms.html#tab:eight-high-schools">4.2</a>， Gelman 和 Carlin 等 （2003年） <span class="citation">(Gelman et al. <a href="#ref-Gelman2003">2003</a>)</span> 建立分层正态模型 <a href="algorithms.html#eq:hierarchical-normal-models">(4.31)</a> 分析 Eight Schools 数据集，由美国教育考试服务调查搜集，用以分析不同的培训项目对学生考试分数的影响，其随机调查了 8 所高中，学生的成绩作为培训效应的估计 <span class="math inline">\(y_j\)</span>，其样本方差 <span class="math inline">\(\sigma^2_j\)</span>，数据集见表 <a href="algorithms.html#tab:eight-high-schools">4.2</a>。这里再次以该数据集和模型为例介绍 Stan 的使用。</p>
<p><span class="math display" id="eq:hierarchical-normal-models">\[\begin{equation}
\begin{aligned}
    &amp;{} \mu \sim \mathcal{N}(0,5), \quad \tau \sim \text{Half-Cauchy}(0,5), \quad p(\mu,\tau) \propto 1 \\
    &amp;{} \eta_i \sim \mathcal{N}(0,1), \quad
        \theta_i  =   \mu + \tau \cdot \eta_i, \quad 
        y_i \sim \mathcal{N}(\theta_i,\sigma^2_{i}), i = 1,\ldots,8
\end{aligned}
\tag{4.31}
\end{equation}\]</span></p>
<table>
<caption><span id="tab:eight-high-schools">表 4.2: </span> Eight Schools 数据集</caption>
<thead>
<tr class="header">
<th align="center">School</th>
<th align="center">A</th>
<th align="center">B</th>
<th align="center">C</th>
<th align="center">D</th>
<th align="center">E</th>
<th align="center">F</th>
<th align="center">G</th>
<th align="center">H</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(y_i\)</span></td>
<td align="center">28</td>
<td align="center">8</td>
<td align="center">-3</td>
<td align="center">7</td>
<td align="center">-1</td>
<td align="center">1</td>
<td align="center">18</td>
<td align="center">12</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\sigma_i\)</span></td>
<td align="center">15</td>
<td align="center">10</td>
<td align="center">16</td>
<td align="center">11</td>
<td align="center">9</td>
<td align="center">11</td>
<td align="center">10</td>
<td align="center">18</td>
</tr>
</tbody>
</table>
<p>根据公式组 <a href="algorithms.html#eq:hierarchical-normal-models">(4.31)</a> 指定的各参数先验分布，分层正态模型可以在 Stan 中写成四段。第一段提供数据：学校的数目 <span class="math inline">\(J\)</span>，估计值 <span class="math inline">\(y_1,\ldots,y_{J}\)</span>，标准差 <span class="math inline">\(\sigma_1,\ldots,\sigma_{J}\)</span>，数据类型可以是整数、实数，结构可以是向量，或更一般的数组，还可以带约束，如在这个模型中 <span class="math inline">\(J\)</span> 限制为非负， <span class="math inline">\(\sigma_{J}\)</span> 必须是正的，另外两个反斜杠 // 表示注释。第二段代码声明参数：模型中的待估参数，学校总体的效应 <span class="math inline">\(\theta_j\)</span>，均值 <span class="math inline">\(\mu\)</span>，标准差 <span class="math inline">\(\tau\)</span>，学校水平上的误差 <span class="math inline">\(\eta\)</span> 和效应 <span class="math inline">\(\theta\)</span>。在这个模型中，用 <span class="math inline">\(\mu,\tau,\eta\)</span> 表示 <span class="math inline">\(\theta\)</span> 而不是直接声明 <span class="math inline">\(\theta\)</span> 作一个参数，通过这种参数化，采样器的运行效率会提高，还应该尽量使用向量化操作代替 for 循环语句。最后一段是模型：稍微注意的是，正文中正态分布 <span class="math inline">\(\mathcal{N}(\cdot,\cdot)\)</span> 中后一个位置是方差，而 Stan 代码中使用的是标准差。<code>target += normal_lpdf(y | theta, sigma)</code> 和 <code>y ~ normal(theta, sigma)</code> 对模型的贡献是一样的，都使用正态分布的对数概率密度函数，只是后者扔掉了对数后验密度的常数项而已，这对于 Stan 的采样、近似和优化算法没有影响。</p>
<p>算法运行的硬件环境是 16 核 32 线程主频 2.8 GHz 英特尔至强 E5-2680 处理器，系统环境 CentOS 7，R 软件版本 3.5.1，RStan 版本 2.17.3。算法参数设置了 4 条迭代链，每条链迭代 10000 次，为复现模型结果随机数种子设为 2018。</p>
<p>分层正态模型<a href="algorithms.html#eq:hierarchical-normal-models">(4.31)</a> 的参数 <span class="math inline">\(\mu,\tau\)</span>，及其参数化引入的中间参数 <span class="math inline">\(\eta_i,\theta_i,i=1,\ldots,8\)</span>，还有对数后验 <span class="math inline">\(\mathrm{lp}\_\_\)</span> 的估计值见表 <a href="algorithms.html#tab:eight-schools-output">4.3</a>。</p>
<table>
<caption><span id="tab:eight-schools-output">表 4.3: </span> 对 Eight Schools 数据集建立分层正态模型 <a href="algorithms.html#eq:hierarchical-normal-models">(4.31)</a>，采用 HMC 算法估计模型各参数值</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mean</th>
<th align="right">se_mean</th>
<th align="right">sd</th>
<th align="right">2.5%</th>
<th align="right">25%</th>
<th align="right">50%</th>
<th align="right">75%</th>
<th align="right">97.5%</th>
<th align="right">n_eff</th>
<th align="right">Rhat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="right">7.99</td>
<td align="right">0.05</td>
<td align="right">5.02</td>
<td align="right">-1.65</td>
<td align="right">4.75</td>
<td align="right">7.92</td>
<td align="right">11.15</td>
<td align="right">18.10</td>
<td align="right">8455</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\tau\)</span></td>
<td align="right">6.47</td>
<td align="right">0.06</td>
<td align="right">5.44</td>
<td align="right">0.22</td>
<td align="right">2.45</td>
<td align="right">5.18</td>
<td align="right">9.07</td>
<td align="right">20.50</td>
<td align="right">7375</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta_1\)</span></td>
<td align="right">0.40</td>
<td align="right">0.01</td>
<td align="right">0.93</td>
<td align="right">-1.49</td>
<td align="right">-0.21</td>
<td align="right">0.42</td>
<td align="right">1.02</td>
<td align="right">2.19</td>
<td align="right">16637</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\eta_2\)</span></td>
<td align="right">0.00</td>
<td align="right">0.01</td>
<td align="right">0.87</td>
<td align="right">-1.73</td>
<td align="right">-0.58</td>
<td align="right">0.00</td>
<td align="right">0.57</td>
<td align="right">1.70</td>
<td align="right">16486</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta_3\)</span></td>
<td align="right">-0.20</td>
<td align="right">0.01</td>
<td align="right">0.93</td>
<td align="right">-1.99</td>
<td align="right">-0.82</td>
<td align="right">-0.20</td>
<td align="right">0.41</td>
<td align="right">1.66</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\eta_4\)</span></td>
<td align="right">-0.04</td>
<td align="right">0.01</td>
<td align="right">0.88</td>
<td align="right">-1.80</td>
<td align="right">-0.60</td>
<td align="right">-0.04</td>
<td align="right">0.53</td>
<td align="right">1.74</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta_5\)</span></td>
<td align="right">-0.36</td>
<td align="right">0.01</td>
<td align="right">0.88</td>
<td align="right">-2.06</td>
<td align="right">-0.94</td>
<td align="right">-0.38</td>
<td align="right">0.20</td>
<td align="right">1.42</td>
<td align="right">15489</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\eta_6\)</span></td>
<td align="right">-0.22</td>
<td align="right">0.01</td>
<td align="right">0.90</td>
<td align="right">-1.96</td>
<td align="right">-0.82</td>
<td align="right">-0.23</td>
<td align="right">0.37</td>
<td align="right">1.57</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\eta_7\)</span></td>
<td align="right">0.34</td>
<td align="right">0.01</td>
<td align="right">0.89</td>
<td align="right">-1.49</td>
<td align="right">-0.24</td>
<td align="right">0.36</td>
<td align="right">0.93</td>
<td align="right">2.04</td>
<td align="right">16262</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\eta_8\)</span></td>
<td align="right">0.05</td>
<td align="right">0.01</td>
<td align="right">0.94</td>
<td align="right">-1.81</td>
<td align="right">-0.57</td>
<td align="right">0.06</td>
<td align="right">0.69</td>
<td align="right">1.91</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\theta_1\)</span></td>
<td align="right">11.45</td>
<td align="right">0.08</td>
<td align="right">8.27</td>
<td align="right">-1.86</td>
<td align="right">6.07</td>
<td align="right">10.27</td>
<td align="right">15.50</td>
<td align="right">31.68</td>
<td align="right">11788</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\theta_2\)</span></td>
<td align="right">7.93</td>
<td align="right">0.04</td>
<td align="right">6.15</td>
<td align="right">-4.45</td>
<td align="right">3.99</td>
<td align="right">7.90</td>
<td align="right">11.74</td>
<td align="right">20.44</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\theta_3\)</span></td>
<td align="right">6.17</td>
<td align="right">0.06</td>
<td align="right">7.67</td>
<td align="right">-11.17</td>
<td align="right">2.07</td>
<td align="right">6.74</td>
<td align="right">10.89</td>
<td align="right">19.94</td>
<td align="right">16041</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\theta_4\)</span></td>
<td align="right">7.66</td>
<td align="right">0.05</td>
<td align="right">6.51</td>
<td align="right">-5.63</td>
<td align="right">3.75</td>
<td align="right">7.72</td>
<td align="right">11.62</td>
<td align="right">20.78</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\theta_5\)</span></td>
<td align="right">5.13</td>
<td align="right">0.05</td>
<td align="right">6.41</td>
<td align="right">-9.51</td>
<td align="right">1.37</td>
<td align="right">5.66</td>
<td align="right">9.43</td>
<td align="right">16.41</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\theta_6\)</span></td>
<td align="right">6.14</td>
<td align="right">0.05</td>
<td align="right">6.66</td>
<td align="right">-8.63</td>
<td align="right">2.35</td>
<td align="right">6.58</td>
<td align="right">10.40</td>
<td align="right">18.47</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\theta_7\)</span></td>
<td align="right">10.64</td>
<td align="right">0.05</td>
<td align="right">6.76</td>
<td align="right">-1.14</td>
<td align="right">6.11</td>
<td align="right">10.11</td>
<td align="right">14.52</td>
<td align="right">25.88</td>
<td align="right">20000</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\theta_8\)</span></td>
<td align="right">8.42</td>
<td align="right">0.06</td>
<td align="right">7.86</td>
<td align="right">-7.24</td>
<td align="right">3.91</td>
<td align="right">8.26</td>
<td align="right">12.60</td>
<td align="right">25.24</td>
<td align="right">16598</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">lp__</td>
<td align="right">-39.55</td>
<td align="right">0.03</td>
<td align="right">2.64</td>
<td align="right">-45.41</td>
<td align="right">-41.15</td>
<td align="right">-39.31</td>
<td align="right">-37.67</td>
<td align="right">-35.12</td>
<td align="right">6325</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>表 <a href="algorithms.html#tab:eight-schools-output">4.3</a> 的列为后验量的估计值：依次是后验均值 <span class="math inline">\(\mathsf{E}(\mu|Y)\)</span>、 蒙特卡罗标准误（Monte Carlo standard error）、后验标准差 （standard deviation） <span class="math inline">\(\mathsf{E}(\sigma|Y)\)</span> 、后验分布的 5 个分位点、有效样本数 <span class="math inline">\(n_{eff}\)</span> 和潜在尺度缩减因子 （potential scale reduction factor），最后两个量 用来分析采样效率和评估迭代序列的平稳性；最后一行表示每次迭代的未正则的对数后验密度 （unnormalized log-posterior density） <span class="math inline">\(\hat{R}\)</span>，当链条都收敛到同一平稳分布的时候，<span class="math inline">\(\hat{R}\)</span> 接近 1。</p>
<p>这里对 <span class="math inline">\(\tau\)</span> 采用的非信息先验是均匀先验，参数 <span class="math inline">\(\tau\)</span> 的 95% 的置信区间是 <span class="math inline">\((0.22,20.5)\)</span>， 数据支持 <span class="math inline">\(\tau\)</span> 的范围低于 20.5。</p>
<div class="figure" style="text-align: center"><span id="fig:posterior-mu-tau"></span>
<img src="figures/posterior_mu_tau.png" alt="对 $\mu,\tau$ 给定均匀先验，后验均值 $\mu$ 和标准差 $\tau$ 的直方图" width="70%" />
<p class="caption">
图 4.1: 对 <span class="math inline">\(\mu,\tau\)</span> 给定均匀先验，后验均值 <span class="math inline">\(\mu\)</span> 和标准差 <span class="math inline">\(\tau\)</span> 的直方图
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:diagnostic"></span>
<img src="figures/trace_mu_log_tau.png" alt="诊断参数$\mu,\log(\tau)$迭代序列的平稳性" width="70%" /><img src="figures/mcmc_mean_tau_div.png" alt="诊断参数$\mu,\log(\tau)$迭代序列的平稳性" width="70%" />
<p class="caption">
图 4.2: 诊断参数<span class="math inline">\(\mu,\log(\tau)\)</span>迭代序列的平稳性
</p>
</div>
<p>为了得到可靠的后验估计，做出合理的推断，诊断序列的平稳性是必不可少的部分，前 5000 次迭代作为 预处理阶段，后 5000 次迭代用作参数的推断，图 <a href="algorithms.html#fig:posterior-mu-tau">4.1</a> (a) 给出 <span class="math inline">\(\mu\)</span> 和 <span class="math inline">\(\log(\tau)\)</span> 的迭代序列图，其中橘黄色线分别是对应的后验均值（表 <a href="algorithms.html#tab:eight-schools-output">4.3</a>的第一列），图 <a href="algorithms.html#fig:posterior-mu-tau">4.1</a> (b) 分别给出 <span class="math inline">\(\log(\tau)\)</span> 的蒙特卡罗误差，图中显示随着迭代次数增加，蒙特卡罗误差趋于稳定，说明参数 <span class="math inline">\(\tau\)</span> 的迭代序列达到平稳分布，即迭代点列可以看作来自参数的后验分布的样本。为了评估链条之间和内部的混合效果，Gelman 等 <span class="citation">(Gelman et al. <a href="#ref-Gelman2013R">2013</a>)</span> 使用潜在尺度缩减因子 （potential scale reduction factor） <span class="math inline">\(\hat{R}\)</span> 描述链条的波动程度，类似一组数据的方差含义，方差越小波动性越小，数据越集中，这里意味着链条波动性小。一般地，对于每个待估的量 <span class="math inline">\(\omega\)</span>，模拟产生 <span class="math inline">\(m\)</span> 条链，每条链有 <span class="math inline">\(n\)</span> 次迭代值 <span class="math inline">\(\omega_{ij} (i = 1,\ldots,n;j=1,\ldots,m)\)</span>，用 <span class="math inline">\(B\)</span> 和 <span class="math inline">\(W\)</span> 分别表示链条之间（不妨看作组间方差）和内部的方差（组内方差）</p>
<p><span class="math display" id="eq:potential-scale-reduction">\[\begin{equation}
\begin{aligned}
&amp; B = \frac{n}{m-1}\sum_{j=1}^{m}(\bar{\omega}_{.j} - \bar{\omega}_{..} ), \quad \bar{\omega}_{.j} = \frac{1}{n}\sum_{i=1}^{n}\omega_{ij}, \quad \bar{\omega}_{..} = \frac{1}{m}\sum_{j=1}^{m} \bar{\omega}_{.j}\\
&amp; W = \frac{1}{m}\sum_{j=1}^{m}s^{2}_{j}, \quad s^{2}_{j} = \frac{1}{n-1}\sum_{i=1}^{n}(\omega_{ij} - \bar{\omega}_{.j})^2
\end{aligned} \tag{4.32}
\end{equation}\]</span></p>
<p><span class="math inline">\(\omega\)</span> 的后验方差 <span class="math inline">\(\widehat{\mathsf{Var}}^{+}(\omega|Y)\)</span> 是 <span class="math inline">\(W\)</span> 和 <span class="math inline">\(B\)</span> 的加权平均</p>
<p><span class="math display">\[\begin{equation}
\widehat{\mathsf{Var}}^{+}(\omega|Y) = \frac{n-1}{n} W + \frac{1}{n} B 
\end{equation}\]</span></p>
<p>当初始分布发散时，这个量会高估边际后验方差，但在链条平稳或 <span class="math inline">\(n \to \infty\)</span> 时，它是无偏的。同时，对任意有限的 <span class="math inline">\(n\)</span>，组内方差 <span class="math inline">\(W\)</span> 应该会低估 <span class="math inline">\(\mathsf{Var}(\omega|Y)\)</span>，因为单个链条没有时间覆盖目标分布；在 <span class="math inline">\(n \to \infty\)</span>， <span class="math inline">\(W\)</span> 的期望会是 <span class="math inline">\(\mathsf{Var}(\omega|Y)\)</span>。</p>
<p>通过迭代序列采集的样本估计 <span class="math inline">\(\hat{R}\)</span> 以检测链条的收敛性</p>
<p><span class="math display">\[\begin{equation}
\hat{R} = \sqrt{\frac{\widehat{\mathsf{Var}}^{+}(\omega|Y)}{W}}
\end{equation}\]</span></p>
<p>随着 <span class="math inline">\(n \to \infty\)</span>， <span class="math inline">\(\hat{R}\)</span> 下降到 1。如果 <span class="math inline">\(\hat{R}\)</span> 比较大，我们有理由认为需要增加模拟次数以改进待估参数 <span class="math inline">\(\omega\)</span> 的后验分布。从表 <a href="algorithms.html#tab:eight-schools-output">4.3</a> 来看，各参数的 <span class="math inline">\(\hat{R}\)</span> 值都是 1，说明各个迭代链混合得好。</p>
</div>
<div id="subsec:stan-hmc" class="section level3">
<h3><span class="header-section-number">4.5.4</span> 实现过程</h3>
<p>为了与本章第 <a href="algorithms.html#sec:MCMC">4.4.3</a> 节提出的贝叶斯 Langevin-Hastings 算法比较，我们基于 Stan 实现求解 SGLMM 模型的贝叶斯汉密尔顿蒙特卡罗算法（简称 Stan-HMC）。目前，我与 Bürkner 一起开发了 brms 包 <span class="citation">(Bürkner <a href="#ref-brms2017JSS">2017</a>)</span>， 主要工作是修复程序调用和文档书写错误， 特别是与求解 SGLMM 模型相关的 <code>gp</code> 函数，相关细节见 brms 的 Github 开发仓库。</p>
<p>在 SGLMM 模型下，Stan-HMC 算法，先从条件分布 <span class="math inline">\(S|\boldsymbol{\theta},\boldsymbol{\beta},Y\)</span> 抽样，然后从条件分布 <span class="math inline">\(\boldsymbol{\theta}|S\)</span> 抽样，最后从条件分布 <span class="math inline">\(\boldsymbol{\beta}|S,Y\)</span> 抽样，具体步骤如下：</p>
<ol style="list-style-type: decimal">
<li>选择初始值 <span class="math inline">\(\boldsymbol{\theta},\boldsymbol{\beta},S\)</span>，如 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的初始值来自正态分布，<span class="math inline">\(\boldsymbol{\theta}\)</span> 的初始值来自对数正态分布；</li>
<li>更新参数向量 <span class="math inline">\(\boldsymbol{\theta}\)</span> ：
<ol style="list-style-type: lower-roman">
<li>从指定的先验分布中均匀抽取新的 <span class="math inline">\(\boldsymbol{\theta}&#39;\)</span> ；</li>
<li>以概率 <span class="math inline">\(\Delta(\boldsymbol{\theta},\boldsymbol{\theta}&#39;) = \min \big\{\frac{p(S|\boldsymbol{\theta}&#39;)}{p(S|\boldsymbol{\theta})},1\big\}\)</span>接受<span class="math inline">\(\boldsymbol{\theta}&#39;\)</span>，否则不改变 <span class="math inline">\(\boldsymbol{\theta}\)</span>。</li>
</ol></li>
<li>更新高斯过程 <span class="math inline">\(S\)</span> 的取值：
<ol style="list-style-type: lower-roman">
<li>抽取新的值 <span class="math inline">\(S_{i}&#39;\)</span>， 向量 <span class="math inline">\(S\)</span> 的第 <span class="math inline">\(i\)</span> 值来自一元条件高斯密度 <span class="math inline">\(p(S_{i}&#39;|S_{-i},\boldsymbol{\theta})\)</span>，<span class="math inline">\(S_{-i}&#39;\)</span> 表示移除 <span class="math inline">\(S\)</span> 中的第 <span class="math inline">\(i\)</span> 个值；</li>
<li>以概率 <span class="math inline">\(\Delta(S_{i},S_{i}&#39;) = \min\big\{ \frac{p(y_{i}|s_{i}&#39;,\boldsymbol{\beta})}{p(y_{i}s_{i},\boldsymbol{\beta})},1 \big\}\)</span> 接受 <span class="math inline">\(S_{i}&#39;\)</span>，否则不改变<span class="math inline">\(S_i\)</span>；</li>
<li>重复 (i) 和 (ii) <span class="math inline">\(\forall i = 1,2,\ldots,n\)</span>。</li>
</ol></li>
<li>更新模型系数 <span class="math inline">\(\boldsymbol{\beta}\)</span> ：从条件密度 <span class="math inline">\(p(\boldsymbol{\beta}&#39;|\boldsymbol{\beta})\)</span> 以概率 <span class="math display">\[\Delta = \min \big\{ \frac{\prod_{j=1}^{n}p(y_i|s_{i},\boldsymbol{\beta}&#39;)p(\boldsymbol{\beta}|\boldsymbol{\beta}&#39;)}{\prod_{j=1}^{n}p(y_i|s_{i},\boldsymbol{\beta})p(\boldsymbol{\beta}&#39;|\boldsymbol{\beta})},1  \big\}\]</span> 接受 <span class="math inline">\(\boldsymbol{\beta}&#39;\)</span>，否则不改变<span class="math inline">\(\boldsymbol{\beta}\)</span>；</li>
<li>重复步骤2，3，4 既定的次数，获得参数 <span class="math inline">\(\boldsymbol{\beta},\boldsymbol{\theta}\)</span> 的迭代序列，直到参数的迭代序列平稳，然后根据后续的平稳序列采样，获得各参数后验分布的样本，再根据样本估计参数值。</li>
</ol>
<p>程序实现的主要步骤（以 R 语言接口 rstan 和 brms 为例说明）：首先安装 C++ 编译工具，如果在 Windows 平台上，就从 R 官网下载安装 <a href="https://cran.r-project.org/bin/windows/Rtools/">RTools</a>， 它包含一套完整的 C++ 开发工具。 然后添加 gcc/g++ 编译器的路径到系统环境变量。 如果在 Linux 系统上， 这些工具都是自带的， 环境变量也不用配置， 减少了很多麻烦，但是在 Linux 系统上可以获得更好的算法性能，其它配置细节见 Stan 开发<a href="https://github.com/stan-dev/rstan/wiki">官网</a>。然后，在 R 软件控制台安装 rstan 和 brms 包以及相关依赖包。最后，加载 rstan 和 brms 包，设置启动参数如下：</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 加载程序包</span>
<span class="kw">library</span>(rstan)
<span class="kw">library</span>(brms)
<span class="co"># 以并行方式运行Stan-HMC算法，指定 CPU 的核心数</span>
<span class="kw">options</span>(<span class="dt">mc.cores =</span> parallel<span class="op">::</span><span class="kw">detectCores</span>())
<span class="co"># 将编译后的模型写入磁盘，可防止重新编译</span>
<span class="kw">rstan_options</span>(<span class="dt">auto_write =</span> <span class="ot">TRUE</span>)</code></pre>
<p>接着调用 brms 包的 <code>brm</code> 函数</p>
<pre class="sourceCode r"><code class="sourceCode r">fit.binomal &lt;-<span class="st"> </span><span class="kw">brm</span>(<span class="dt">formula =</span> y <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(units.m) <span class="op">~</span><span class="st"> </span>
<span class="st">  </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>intercept <span class="op">+</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span><span class="kw">gp</span>(d1, d2), 
  <span class="dt">data =</span> sim_binom_data,
  <span class="dt">prior =</span> <span class="kw">set_prior</span>(<span class="st">&quot;normal(0,10)&quot;</span>, <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),
  <span class="dt">chains =</span> <span class="dv">4</span>, <span class="dt">thin =</span> <span class="dv">5</span>, <span class="dt">iter =</span> <span class="dv">15000</span>, <span class="dt">family =</span> <span class="kw">binomial</span>()
)    </code></pre>
<p><code>brm</code> 函数可设置的参数有几十个，下面仅列出部分</p>
<ol style="list-style-type: decimal">
<li><code>formula</code> ：设置 SGLMM 模型的结构，其中波浪号左侧是响应变量，<code>trials</code> 表示在每个采样点抽取的样本量；波浪号右侧 <code>0 + intercept</code> 表示截距项， <code>x1</code> 和 <code>x2</code> 表示协变量，<code>gp(d1, d2)</code> 表示采样坐标为 <code>(d1,d2)</code> 自相关函数为幂指数族的平稳高斯过程</li>
<li><code>data</code> ： SGLMM 模型拟合的数据<code>sim_binom_data</code></li>
<li><code>prior</code> ： 设置 SGLMM 模型参数的先验分布</li>
<li><code>chains</code> ： 指定同时生成马尔科夫链的数目</li>
<li><code>iter</code> ： 算法总迭代次数</li>
<li><code>thin</code> ： <code>burn-in</code> 位置之后，每隔 <code>thin</code> 的间距就采一个样本</li>
<li><code>family</code> : 指定响应变量服从的分布，如二项分布，泊松分布等</li>
</ol>
</div>
</div>
<div id="subsec:sglmm-with-r" class="section level2">
<h2><span class="header-section-number">4.6</span> 实现参数估计的 R 包</h2>
<p>R 语言作为免费自由的统计计算和绘图环境，因其更新快，社区庞大，扩展包更是超过了 13000 个，提供了大量前沿统计方法的代码实现。如 spBayes 包使用贝叶斯 MCMC 算法估计 SGLMM 模型的参数 <span class="citation">(Finley, Banerjee, and E.Gelfand <a href="#ref-spBayes2015">2015</a>)</span>； coda 包诊断马尔科夫链的平稳性 <span class="citation">(Plummer et al. <a href="#ref-coda2006">2006</a>)</span>；MCMCvis 包分析和可视化贝叶斯 MCMC 算法的输出， 提取模型参数， 转化 JAGS、Stan 和 BUGS 软件的输出结果到 R 对象，以利后续分析；geoR 包 在空间线性混合效应模型上基于 Langevin-Hastings 实现了贝叶斯 MCMC 算法 <span class="citation">(Ribeiro Jr. and Diggle <a href="#ref-geoR2001">2001</a>)</span>；geoRglm 包在 geoR 包的基础上将模型范围扩展到 SGLMM 模型 <span class="citation">(Christensen and Ribeiro Jr. <a href="#ref-geoRglm2002">2002</a>)</span>；glmmBUGS 包提供了 WinBUGS、 OpenBUGS 和 JAGS 软件的统一接口 <span class="citation">(Brown and Zhou <a href="#ref-glmmBUGS2010MCMC">2010</a>)</span>。 目前，R 语言社区提供的求解 SGLMM 模型的 R 包和功能实现，见表 <a href="algorithms.html#tab:sglmm-packages">4.4</a>。</p>
<table>
<caption><span id="tab:sglmm-packages">表 4.4: </span> 求解空间广义线性混合效应模型的 R 包功能比较：加号 + 表示可用，减号 - 表示不可用，星号 * 标记的只在空间线性混合效应模型下可用</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">PrevMap</th>
<th align="center">geoR</th>
<th align="center">geoRglm</th>
<th align="center">geostatsp</th>
<th align="center">geoBayes</th>
<th align="center">spBayes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">二项空间模型</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="left">基于似然函数推断</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">基于贝叶斯推断</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="left">模型的块金效应</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">+</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">低秩近似算法</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="left">分层模型</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="odd">
<td align="left">非线性预测</td>
<td align="center">+</td>
<td align="center">+*</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
</tr>
<tr class="even">
<td align="left">多元预测</td>
<td align="center">+</td>
<td align="center">+*</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
</tr>
<tr class="odd">
<td align="left">空间过程各向异性</td>
<td align="center">-</td>
<td align="center">+*</td>
<td align="center">+</td>
<td align="center">+*</td>
<td align="center">-</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="left">非梅隆型协方差函数</td>
<td align="center">-</td>
<td align="center">+*</td>
<td align="center">+</td>
<td align="center">-</td>
<td align="center">+</td>
<td align="center">+</td>
</tr>
</tbody>
</table>
</div>
<div id="sec:estimations" class="section level2">
<h2><span class="header-section-number">4.7</span> 本章小结</h2>
<p>本章参数估计和算法实现是论文的主要内容之一，首先沿着极大似然估计的思路，尝试写出 SGLMM 模型参数的似然函数，但是因为空间随机效应导致的高维积分无法用显式表达式表示，进而出现了以拉普拉斯近似和蒙特卡罗模拟的两类基于似然的方法，前者走近似高维积分的路子，后者走模拟计算的路子，这两类方法在数据分析中，前者尤其需要指定合适的初值，且在数据量不太大的时候才能应用，后者只需指定合适的先验分布使得马氏链收敛即可。第<a href="algorithms.html#sec:stan-hmc">4.5</a>节在第<a href="algorithms.html#sec:MCMC">4.4.3</a>小节的基础上提出基于 Stan 实现的汉密尔顿蒙特卡罗算法。</p>

</div>
</div>
<h3>参考文献</h3>
<div id="refs" class="references">
<div id="ref-Natarajan1995">
<p>Natarajan, Ranjini, and Charles E. McCulloch. 1995. “A Note on the Existence of the Posterior Distribution for a Class of Mixed Models for Binomial Responses.” <em>Biometrika</em> 82 (3): 639–43.</p>
</div>
<div id="ref-Robert1996JASA">
<p>Kass, Robert E., and Larry Wasserman. 1996. “The Selection of Prior Distributions by Formal Rules.” <em>Journal of the American Statistical Association</em> 91 (435): 1343–70.</p>
</div>
<div id="ref-Diggle2007">
<p>Diggle, Peter J., and Paulo J. Ribeiro Jr. n.d. <em>Model-Based Geostatistics</em>. New York, NY: Springer-Verlag.</p>
</div>
<div id="ref-Warnes1987">
<p>Warnes, J. J., and B. D. Ripley. 1987. “Problems with Likelihood Estimation of Covariance Functions of Spatial Gaussian Processes.” <em>Biometrika</em> 74 (3): 640–42.</p>
</div>
<div id="ref-Tierney1986">
<p>Tierney, Luke, and Joseph B. Kadane. 1986. “Accurate Approximations for Posterior Moments and Marginal Densities.” <em>Journal of the American Statistical Association</em> 81 (393): 82–86.</p>
</div>
<div id="ref-Diggle2002Analysis">
<p>Diggle, Peter J., Patrick Heagerty, Kung-Yee Liang, and Scott L. Zeger. n.d. <em>Analysis of Longitudinal Data</em>. Second. New York: Oxford University Press.</p>
</div>
<div id="ref-R-bbmle">
<p>Bolker, Ben, and R Development Core Team. 2017. <em>Bbmle: Tools for General Maximum Likelihood Estimation</em>. <a href="https://CRAN.R-project.org/package=bbmle">https://CRAN.R-project.org/package=bbmle</a>.</p>
</div>
<div id="ref-Huang2016COS">
<p>黄湘云. 2016. “R 语言做符号计算.” <a href="https://cosx.org/2016/07/r-symbol-calculate">https://cosx.org/2016/07/r-symbol-calculate</a>.</p>
</div>
<div id="ref-Bonat2016Practical">
<p>Bonat, Wagner Hugo, and Paulo J. Ribeiro Jr. 2016. “Practical Likelihood Analysis for Spatial Generalized Linear Mixed Models.” <em>Environmetrics</em> 27 (2): 83–89.</p>
</div>
<div id="ref-Geyer1994On">
<p>Geyer, Charles J. 1994. “On the Convergence of Monte Carlo Maximum Likelihood Calculations.” <em>Journal of the Royal Statistical Society, Series B</em> 56 (1): 261–74.</p>
</div>
<div id="ref-PrevMap2017JSS">
<p>Giorgi, Emanuele, and Peter J. Diggle. 2017. “PrevMap: An R Package for Prevalence Mapping.” <em>Journal of Statistical Software</em> 78 (8): 1–29.</p>
</div>
<div id="ref-Christensen2006">
<p>Christensen, Ole F, Gareth O Roberts, and Martin Sköld. 2006. “Robust Markov Chain Monte Carlo Methods for Spatial Generalized Linear Mixed Models.” <em>Journal of Computational and Graphical Statistics</em> 15 (1): 1–17.</p>
</div>
<div id="ref-Brooks2011">
<p>Brooks, Steve, Andrew Gelman, Galin Jones, and XiaoLi Meng. 2011. <em>Handbook of Markov Chain Monte Carlo</em>. Boca Raton, Florida: Chapman; Hall/CRC.</p>
</div>
<div id="ref-Stan2017JSS">
<p>Carpenter, Bob, Andrew Gelman, Matthew Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. “Stan: A Probabilistic Programming Language.” <em>Journal of Statistical Software</em> 76 (1): 1–32.</p>
</div>
<div id="ref-BUGS2009">
<p>Lunn, David, David Spiegelhalter, Andrew Thomas, and Nicky Best. 2009. “The BUGS Project: Evolution, Critique and Future Directions.” <em>Statistics in Medicine</em> 28 (25): 3049–67.</p>
</div>
<div id="ref-Huang2017COS">
<p>黄湘云. 2017. “随机数生成及其在统计模拟中的应用.” <a href="https://cosx.org/2017/05/random-number-generation">https://cosx.org/2017/05/random-number-generation</a>.</p>
</div>
<div id="ref-hoffman2014">
<p>Hoffman, Matthew D., and Andrew Gelman. 2014. “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” <em>Journal of Machine Learning Research</em> 15 (1): 1593–1623.</p>
</div>
<div id="ref-Rubin1981">
<p>Rubin, Donald B. 1981. “Estimation in Parallel Randomized Experiments.” <em>Journal of Educational Statistics</em> 6 (4): 377–401.</p>
</div>
<div id="ref-Alderman1980">
<p>Alderman, Donald L., and Donald E. Powers. 1980. “The Effects of Special Preparation on Sat-Verbal Scores.” <em>American Educational Research Journal</em> 17 (2): 239–51.</p>
</div>
<div id="ref-Gelman2003">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2003. <em>Bayesian Data Analysis</em>. Second. London: Chapman; Hall/CRC.</p>
</div>
<div id="ref-Gelman2013R">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <em>Bayesian Data Analysis</em>. Third. Boca Raton, Florida: Chapman; Hall/CRC.</p>
</div>
<div id="ref-brms2017JSS">
<p>Bürkner, Paul. 2017. “brms: An R Package for Bayesian Multilevel Models Using Stan.” <em>Journal of Statistical Software</em> 80 (1): 1–28.</p>
</div>
<div id="ref-spBayes2015">
<p>Finley, Andrew O., Sudipto Banerjee, and Alan E.Gelfand. 2015. “spBayes for Large Univariate and Multivariate Point-Referenced Spatio-Temporal Data Models.” <em>Journal of Statistical Software</em> 63 (13): 1–28.</p>
</div>
<div id="ref-coda2006">
<p>Plummer, Martyn, Nicky Best, Kate Cowles, and Karen Vines. 2006. “coda: Convergence Diagnosis and Output Analysis for MCMC.” <em>R News</em> 6 (1): 7–11.</p>
</div>
<div id="ref-geoR2001">
<p>Ribeiro Jr., Paulo J., and Peter J. Diggle. 2001. “geoR: A Package for Geostatistical Analysis.” <em>R News</em> 1 (2): 14–18.</p>
</div>
<div id="ref-geoRglm2002">
<p>Christensen, O.F., and P.J. Ribeiro Jr. 2002. “geoRglm: A Package for Generalised Linear Spatial Models.” <em>R News</em> 2 (2): 26–28.</p>
</div>
<div id="ref-glmmBUGS2010MCMC">
<p>Brown, Patrick E, and L Zhou. 2010. “MCMC for Generalized Linear Mixed Models with glmmBUGS.” <em>R Journal</em> 2 (1): 13–17.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>SGLMM 模型的似然函数通常不止一个极值点<a href="algorithms.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simulations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/XiangyunHuang/Thesis-Template-Bookdown/edit/master/03-estimations.Rmd",
"text": "缂栬緫"
},
"history": {
"link": null,
"text": null
},
"download": ["Thesis-Template-Bookdown.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
